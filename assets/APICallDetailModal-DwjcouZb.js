import{d as e,a3 as n,W as a,Y as t,a7 as s,F as r,a5 as l,_ as i,k as o,U as p,X as c,G as d,u as m,ad as u,f,c as v,w as g,o as _,S as y,a6 as h,B as b,ab as k}from"./vendor-DCf9YP3r.js";import{u as x,b as P,a as C,P as w}from"./designs-db-D4FnITPe.js";import{u as I}from"./variables-db-BMGRJyAs.js";import{u as S}from"./models-db-3pG6m6YE.js";import{u as A,T as E,a as O,b as T}from"./trials-db-V2RTzc2h.js";import{p as N}from"./registry-DiQCbBcP.js";import{u as R}from"./settings-db-CBuX-T7S.js";import{g as M}from"./defaultData-DyqJdH2z.js";import{M as j}from"./ModelSelectionTable-svkIGbxX.js";import{P as $,F,_ as U,E as D,Q as q,A as L,f as z}from"./index-dQqVkRMY.js";import{c as B}from"./cost-formatting-Cno2rTOg.js";import{_ as K}from"./BaseModal.vue_vue_type_style_index_0_lang-Br6W_Dvv.js";const Y={class:"design-selector"},G={class:"designs-list"},J=["onClick"],H={class:"design-main"},V={class:"design-name"},W={class:"design-details"},X={key:0,class:"design-description"},Q={key:1,class:"template-preview"},Z={class:"design-meta"},ee={class:"meta-item"},ne={class:"meta-item"},ae={class:"design-main"},te={class:"design-name"},se={key:0,class:"empty-state"},re={key:1,class:"loading-state"},le=U(e({__name:"DesignSelector",props:{designs:{},loading:{type:Boolean,default:!1}},emits:["select","create"],setup(e){function u(e){return Object.keys(e.variableBindings||{}).length}function f(e){if(!e.variableBindings)return"1";let n=1;for(const a of Object.values(e.variableBindings))if("direct"===a.type&&a.values)n*=a.values.length;else if("list"===a.type)return"?";return n>1e3?`${(n/1e3).toFixed(1)}k`:n.toString()}function v(e,n=150){return e.length<=n?e:e.substring(0,n)+"..."}return(e,g)=>{const _=p("a-tag"),y=p("a-button"),h=p("a-spin");return a(),n("div",Y,[g[7]||(g[7]=t("div",{class:"selector-header"},[t("h3",null,"Available Designs"),t("p",{class:"selector-subtitle"},"Select a design or create a new one")],-1)),t("div",G,[(a(!0),n(r,null,l(e.designs,s=>{return a(),n("div",{key:s.id,class:"design-row",onClick:n=>e.$emit("select",s.id)},[t("div",H,[t("div",V,i(s.name),1),t("div",W,[s.description?(a(),n("span",X,i(s.description),1)):(a(),n("span",Q,i(v(s.promptTemplate,80)),1))])]),t("div",Z,[o(_,{size:"small",color:(r=s.outputType,{text:"blue",number:"green",boolean:"purple"}[r]||"default")},{default:c(()=>[d(i(s.outputType),1)]),_:2},1032,["color"]),t("span",ee,i(u(s))+" vars",1),t("span",ne,i(f(s))+" combos",1)])],8,J);var r}),128)),t("div",{class:"design-row create-row",onClick:g[0]||(g[0]=n=>e.$emit("create"))},[t("div",ae,[t("div",te,[o(m($)),g[2]||(g[2]=d(" Create New Design "))]),g[3]||(g[3]=t("div",{class:"design-details"}," Start with a blank template ",-1))])])]),0!==e.designs.length||e.loading?s("",!0):(a(),n("div",se,[o(m(F),{class:"empty-icon"}),g[5]||(g[5]=t("h4",null,"No designs available",-1)),g[6]||(g[6]=t("p",null,"Create your first design to get started",-1)),o(y,{type:"primary",onClick:g[1]||(g[1]=n=>e.$emit("create"))},{default:c(()=>g[4]||(g[4]=[d(" Create Design ")])),_:1,__:[4]})])),e.loading?(a(),n("div",re,[o(h,{size:"large"})])):s("",!0)])}}}),[["__scopeId","data-v-4b6e1fb8"]]),ie={class:"modal-footer"},oe={key:0,class:"footer-left"},pe={class:"footer-actions"},ce=U(e({__name:"ModalFooter",setup:e=>(e,r)=>(a(),n("div",ie,[e.$slots.left?(a(),n("div",oe,[u(e.$slots,"left",{},void 0,!0)])):s("",!0),t("div",pe,[u(e.$slots,"default",{},void 0,!0)])]))}),[["__scopeId","data-v-3a3879d0"]]),de={class:"trial-top-bar"},me={class:"top-bar-left"},ue={key:0,class:"header-label"},fe={key:1,class:"variable-summary"},ve={class:"trial-modal-content"},ge={class:"modal-main"},_e={class:"left-panel"},ye={key:0,class:"design-selector-container"},he={key:1,class:"guide-state"},be={key:2,class:"section dense-section"},ke={key:0,class:"param-section"},xe={key:5,class:"text-secondary text-xs"},Pe={key:6,class:"param-description text-secondary"},Ce={class:"param-section"},we={class:"response-mode-option-compact"},Ie={class:"mode-label"},Se={class:"mode-desc"},Ae={key:1,class:"param-section"},Ee={key:5,class:"text-secondary text-xs"},Oe={key:6,class:"param-description text-secondary"},Te={class:"param-section"},Ne={class:"api-preview"},Re={class:"action-section"},Me={class:"right-panel"},je={class:"section dense-section"},$e={key:0,class:"cost-calculation-compact"},Fe={class:"cost-row"},Ue={key:0,class:"cost-total"},De={class:"bottom-panel"},qe={class:"section dense-section"},Le={key:1,class:"config-list-compact"},ze={class:"config-compact-left"},Be={class:"config-name"},Ke={class:"config-model"},Ye={key:0,class:"config-param"},Ge={key:1,class:"config-param"},Je={class:"config-compact-right"},He={class:"config-total-cost"},Ve={key:2,class:"repeat-config-section"},We={key:3,class:"trial-summary-compact"},Xe={class:"summary-row"},Qe={class:"summary-item"},Ze={class:"summary-item"},en={class:"summary-item"},nn={class:"text-primary"},an={key:0,class:"text-secondary",style:{"font-weight":"normal"}},tn={class:"summary-item"},sn={class:"text-success"},rn=U(e({__name:"TrialCreationModal",props:{trialToDuplicate:{},initialDesignId:{}},emits:["close","created"],setup(e,{emit:u}){const b=e,k=u,w=x(),E=I(),O=S(),T=A(),$=R(),F=f(""),U=f(b.initialDesignId||""),L=f(null),z=f({}),K=f("text"),Y=f([]),G=f({callsPerPrompt:1,strategy:"sequential",delayBetweenRepeats:0,aggregation:"all"}),J=v(()=>w.designs.map(e=>({...e,variableBindings:Object.fromEntries(Object.entries(e.variableBindings).map(([e,n])=>[e,{...n,values:n.values?[...n.values]:void 0,source:n.source?{...n.source}:void 0}])),refusalWords:e.refusalWords?[...e.refusalWords]:void 0}))),H=v(()=>O.modelsByProvider),V=v(()=>J.value.find(e=>e.id===U.value)),W=v(()=>{if(!V.value?.variableBindings)return 0;let e=1;for(const[,n]of Object.entries(V.value.variableBindings))if("list"===n.type&&n.listId){const a=E.lists.find(e=>e.id===n.listId);e*=a?.itemCount||0}else"direct"===n.type&&n.values&&(e*=n.values.length);return e}),X=v(()=>{const e=[];return Object.entries(H.value).forEach(([,n])=>{const a=n.filter(e=>e.enabled);e.push(...a)}),e}),Q=v(()=>L.value?N.getBasicParameters(L.value.provider,L.value.modelId):{}),Z=v(()=>L.value?N.getAdvancedParameters(L.value.provider,L.value.modelId):{}),ee=v(()=>L.value?N.getResponseModes(L.value.provider):{}),ne=v(()=>{if(!L.value)return"";try{const e=N.applyResponseMode(L.value.provider,K.value,z.value),n=P.buildAPIRequest({id:"preview",name:"Preview",provider:L.value.provider,model:L.value.modelId,params:e,created_at:new Date},"{{prompt}}",$.getApiKey(L.value.provider),$.getBaseUrl(L.value.provider));return JSON.stringify(n.body,null,2)}catch(e){return"Error generating preview"}}),ae=v(()=>F.value.trim().length>0),te=v(()=>""!==U.value),se=v(()=>null!==L.value),re=v(()=>W.value),ie=v(()=>Y.value.length*re.value),oe=v(()=>ie.value*G.value.callsPerPrompt),pe=v(()=>{let e=0;return Y.value.forEach(n=>{const a=un(n);a&&(e+=a*re.value)}),e*G.value.callsPerPrompt}),rn=v(()=>ae.value&&te.value&&Y.value.length>0&&W.value>0),ln=v(()=>{if(!L.value)return null;const e=L.value;if(!e.capabilities?.inputCostPerToken||!e.capabilities?.outputCostPerToken)return null;if(!V.value?.tokenEstimate?.avgTokens)return null;const n=V.value.tokenEstimate.avgTokens,a=C.getOutputTokenLimit(e.provider,e.modelId,z.value||{});return B(n,a,e.capabilities.inputCostPerToken,e.capabilities.outputCostPerToken)});function on(e,n){const a=z.value[e];if(a)try{const t=JSON.parse(a);if("array"===n&&!Array.isArray(t))throw new Error("Value must be an array");if("object"===n&&"object"!=typeof t)throw new Error("Value must be an object");z.value[e]=t}catch(t){console.error(`Invalid JSON for ${e}:`,t)}}function pn(e){L.value=e,z.value={},K.value="text";const n=(e,a="")=>{Object.entries(e).forEach(([e,t])=>{const s=a?`${a}.${e}`:e;"object"===t.type&&t.properties?n(t.properties,s):void 0!==t.default&&(z.value[s]=t.default)})},a=N.getParametersForModel(e.provider,e.modelId);n(a)}function cn(){}function dn(){if(!L.value)return;const e=N.applyResponseMode(L.value.provider,K.value,z.value),n=ee.value[K.value]?.label||K.value;Y.value.push({name:`${L.value.displayName} (${n})`,provider:L.value.provider,modelId:L.value.modelId,parameters:{...e}}),L.value=null,z.value={},K.value="text"}function mn(e){Y.value.splice(e,1)}function un(e){const n=O.getModel(e.provider,e.modelId);if(!n?.capabilities?.inputCostPerToken||!n?.capabilities?.outputCostPerToken)return null;if(!V.value?.tokenEstimate?.avgTokens)return null;const a=V.value.tokenEstimate.avgTokens,t=C.getOutputTokenLimit(n.provider,n.modelId,e.parameters||{});return B(a,t,n.capabilities.inputCostPerToken,n.capabilities.outputCostPerToken)}const fn=h();function vn(){k("close"),fn.push("/designs")}async function gn(){try{const e=await T.createTrial({name:F.value,designId:U.value,configurations:JSON.parse(JSON.stringify(Y.value)),repeatConfig:G.value.callsPerPrompt>1?{...G.value}:void 0});k("created",e),k("close")}catch(e){console.error("Failed to create trial:",e),alert("Failed to create trial: "+(e instanceof Error?e.message:"Unknown error"))}}return g(()=>b.trialToDuplicate,async e=>{if(e){e.designSnapshot,F.value=`${e.name} (Copy)`,await w.initialize();const n=e.designSnapshot?.originalId;U.value=n,Y.value=(e.configurationSnapshots||e.configurations||[]).map(e=>({...e,id:M()}))}},{immediate:!0}),g(U,async e=>{if(!e)return;const n=J.value.find(n=>n.id===e);if(!n)return;if(!n.tokenEstimate&&n.variableBindings&&Object.keys(n.variableBindings).length>0)try{const e=await C.calculateDesignTokens(n);await w.updateDesign(n.id,{tokenEstimate:e})}catch(l){console.warn("Failed to calculate token estimate:",l)}const a=T.trials,t=n.name.toLowerCase(),s=new RegExp(`^${t}\\s+(\\d+)$`,"i");let r=0;for(const i of a){const e=i.name.match(s);if(e){const n=parseInt(e[1]);n>r&&(r=n)}}F.value=`${t} ${r+1}`}),g(()=>b.initialDesignId,e=>{e&&(U.value=e)},{immediate:!0}),_(async()=>{await w.initialize(),await E.initialize(),await T.initialize(),0===O.enabledModels.length&&await O.ensureDefaultsEnabled()}),(e,u)=>{const f=p("a-select-option"),v=p("a-select"),g=p("a-input"),_=p("a-input-number"),h=p("a-switch"),b=p("a-textarea"),k=p("a-form-item"),x=p("a-col"),C=p("a-row"),w=p("a-form"),I=p("a-collapse-panel"),S=p("a-collapse"),A=p("a-button"),E=p("a-empty"),T=p("a-tooltip"),R=p("a-tag"),M=p("a-alert"),$=p("a-modal");return a(),y($,{open:!0,title:null,width:"95vw",style:{top:"2.5vh",maxWidth:"none"},bodyStyle:{height:"95vh",padding:"0",overflow:"hidden"},footer:null,maskClosable:!1,onCancel:u[8]||(u[8]=n=>e.$emit("close"))},{default:c(()=>[t("div",de,[t("div",me,[u[9]||(u[9]=t("span",{class:"create-trial-title"},"Create Trial",-1)),u[10]||(u[10]=t("span",{class:"top-bar-separator"},"|",-1)),u[11]||(u[11]=t("span",{class:"header-label"},"Select Design",-1)),o(v,{value:U.value,"onUpdate:value":u[0]||(u[0]=e=>U.value=e),placeholder:"Select Design",size:"large",class:"design-select","dropdown-class-name":"design-select-dropdown",showSearch:"","filter-option":(e,n)=>(n?.label||n?.value||"").toLowerCase().includes(e.toLowerCase())},{default:c(()=>[(a(!0),n(r,null,l(J.value,e=>(a(),y(f,{key:e.id,value:e.id,label:e.name},{default:c(()=>[d(i(e.name),1)]),_:2},1032,["value","label"]))),128))]),_:1},8,["value","filter-option"]),u[12]||(u[12]=t("span",{class:"header-label"},"Trial Name",-1)),o(g,{value:F.value,"onUpdate:value":u[1]||(u[1]=e=>F.value=e),placeholder:"Auto-generated...",size:"large",class:"trial-name-input"},null,8,["value"]),V.value?(a(),n("span",ue,"Design Variables")):s("",!0),V.value?(a(),n("span",fe," ("+i(W.value)+" conditions × "+i(V.value?.tokenEstimate?.avgTokens||"?")+" tokens/prompt = "+i(W.value*(V.value?.tokenEstimate?.avgTokens||0))+" input tokens) ",1)):s("",!0)]),u[13]||(u[13]=t("div",{class:"top-bar-right"},null,-1))]),t("div",ve,[t("div",ge,[t("div",_e,[U.value?L.value?(a(),n("div",be,[t("h3",null,"Configure: "+i(L.value.displayName),1),Object.keys(Q.value).length>0?(a(),n("div",ke,[u[15]||(u[15]=t("h4",null,"Basic Parameters",-1)),o(w,{layout:"vertical",size:"large"},{default:c(()=>[o(C,{gutter:[8,8]},{default:c(()=>[(a(!0),n(r,null,l(Q.value,(e,t)=>(a(),y(x,{key:t,span:12},{default:c(()=>[o(k,{label:t,class:"dense-form-item"},{default:c(()=>["number"===e.type||"integer"===e.type?(a(),y(_,{key:0,value:z.value[t],"onUpdate:value":e=>z.value[t]=e,min:e.min,max:e.max,step:"integer"===e.type?1:.1,placeholder:String(e.default),size:"large",style:{width:"100%"}},null,8,["value","onUpdate:value","min","max","step","placeholder"])):"string"!==e.type||e.enum?"string"===e.type&&e.enum?(a(),y(v,{key:2,value:z.value[t],"onUpdate:value":e=>z.value[t]=e,size:"large"},{default:c(()=>[(a(!0),n(r,null,l(e.enum,e=>(a(),y(f,{key:e,value:e},{default:c(()=>[d(i(e),1)]),_:2},1032,["value"]))),128))]),_:2},1032,["value","onUpdate:value"])):"boolean"===e.type?(a(),y(h,{key:3,checked:z.value[t],"onUpdate:checked":e=>z.value[t]=e,size:"large"},null,8,["checked","onUpdate:checked"])):"array"===e.type||"object"===e.type?(a(),y(b,{key:4,value:z.value[t],"onUpdate:value":e=>z.value[t]=e,placeholder:"array"===e.type?"[...]":"{...}","auto-size":{minRows:1,maxRows:3},size:"large",onBlur:n=>on(t,e.type)},null,8,["value","onUpdate:value","placeholder","onBlur"])):(a(),n("span",xe,i(e.type)+" not supported ",1)):(a(),y(g,{key:1,value:z.value[t],"onUpdate:value":e=>z.value[t]=e,placeholder:String(e.default),size:"large"},null,8,["value","onUpdate:value","placeholder"])),e.description?(a(),n("small",Pe,i(e.description),1)):s("",!0)]),_:2},1032,["label"])]),_:2},1024))),128))]),_:1})]),_:1})])):s("",!0),t("div",Ce,[u[16]||(u[16]=t("h4",null,"Response Mode",-1)),o(v,{value:K.value,"onUpdate:value":u[3]||(u[3]=e=>K.value=e),onChange:cn,size:"large",style:{width:"100%"}},{default:c(()=>[(a(!0),n(r,null,l(ee.value,(e,n)=>(a(),y(f,{key:n,value:n},{default:c(()=>[t("div",we,[t("span",Ie,i(e.label),1),t("span",Se,i(e.description),1)])]),_:2},1032,["value"]))),128))]),_:1},8,["value"])]),Object.keys(Z.value).length>0?(a(),n("div",Ae,[o(S,{size:"small",ghost:""},{default:c(()=>[o(I,{key:"advanced",header:"Advanced Parameters"},{default:c(()=>[o(w,{layout:"vertical",size:"large"},{default:c(()=>[o(C,{gutter:[8,8]},{default:c(()=>[(a(!0),n(r,null,l(Z.value,(e,t)=>(a(),y(x,{key:t,span:12},{default:c(()=>[o(k,{label:t,class:"dense-form-item"},{default:c(()=>["number"===e.type||"integer"===e.type?(a(),y(_,{key:0,value:z.value[t],"onUpdate:value":e=>z.value[t]=e,min:e.min,max:e.max,step:"integer"===e.type?1:.1,placeholder:String(e.default),size:"large",style:{width:"100%"}},null,8,["value","onUpdate:value","min","max","step","placeholder"])):"string"!==e.type||e.enum?"string"===e.type&&e.enum?(a(),y(v,{key:2,value:z.value[t],"onUpdate:value":e=>z.value[t]=e,size:"large"},{default:c(()=>[(a(!0),n(r,null,l(e.enum,e=>(a(),y(f,{key:e,value:e},{default:c(()=>[d(i(e),1)]),_:2},1032,["value"]))),128))]),_:2},1032,["value","onUpdate:value"])):"boolean"===e.type?(a(),y(h,{key:3,checked:z.value[t],"onUpdate:checked":e=>z.value[t]=e,size:"large"},null,8,["checked","onUpdate:checked"])):"array"===e.type||"object"===e.type?(a(),y(b,{key:4,value:z.value[t],"onUpdate:value":e=>z.value[t]=e,placeholder:"array"===e.type?"[...]":"{...}","auto-size":{minRows:1,maxRows:3},size:"large",onBlur:n=>on(t,e.type)},null,8,["value","onUpdate:value","placeholder","onBlur"])):(a(),n("span",Ee,i(e.type)+" not supported ",1)):(a(),y(g,{key:1,value:z.value[t],"onUpdate:value":e=>z.value[t]=e,placeholder:String(e.default),size:"large"},null,8,["value","onUpdate:value","placeholder"])),e.description?(a(),n("small",Oe,i(e.description),1)):s("",!0)]),_:2},1032,["label"])]),_:2},1024))),128))]),_:1})]),_:1})]),_:1})]),_:1})])):s("",!0),t("div",Te,[o(S,{size:"small",ghost:""},{default:c(()=>[o(I,{key:"preview",header:"Raw API Preview"},{default:c(()=>[t("pre",Ne,i(ne.value),1)]),_:1})]),_:1})]),t("div",Re,[o(A,{type:"primary",onClick:dn,disabled:!se.value,size:"large",block:""},{default:c(()=>u[17]||(u[17]=[d(" Add Configuration ")])),_:1,__:[17]},8,["disabled"])])])):(a(),n("div",he,u[14]||(u[14]=[t("div",{class:"guide-box"},[t("h2",null,"→ CHOOSE A MODEL →"),t("p",null,"Select any model from the right panel")],-1)]))):(a(),n("div",ye,[o(le,{designs:J.value,loading:!1,onSelect:u[2]||(u[2]=e=>U.value=e),onCreate:vn},null,8,["designs"])]))]),t("div",Me,[t("div",je,[u[20]||(u[20]=t("h3",null,"Select Model",-1)),L.value&&ln.value?(a(),n("div",$e,[t("div",Fe,[u[19]||(u[19]=t("span",{class:"cost-label"},"Cost per call:",-1)),t("strong",null,"$"+i(ln.value.toFixed(5)),1),re.value>0?(a(),n("span",Ue,[u[18]||(u[18]=d(" • Total: ")),t("strong",null,"$"+i((ln.value*re.value).toFixed(3)),1),d(" ("+i(re.value)+" calls) ",1)])):s("",!0)])])):s("",!0),o(j,{models:X.value,"selected-model":L.value,"is-loading":!1,"total-calls":W.value,design:V.value,"model-params":z.value,onModelSelected:pn,onModelConfigured:pn},null,8,["models","selected-model","total-calls","design","model-params"])])])]),t("div",De,[t("div",qe,[u[36]||(u[36]=t("h3",null,"Trial Configurations",-1)),0===Y.value.length?(a(),y(E,{key:0,description:"No configurations added yet. Select a model and configure parameters.",style:{margin:"var(--spacing-md) 0"}},{image:c(()=>u[21]||(u[21]=[t("div",{style:{color:"var(--color-text-disabled)","font-size":"var(--font-size-xxl)"}},"⚙️",-1)])),_:1})):(a(),n("div",Le,[(a(!0),n(r,null,l(Y.value,(e,r)=>(a(),n("div",{key:r,class:"config-item-compact"},[t("div",ze,[t("strong",Be,i(e.name),1),u[22]||(u[22]=t("span",{class:"config-separator"},"•",-1)),t("span",Ke,i(e.provider)+":"+i(e.modelId),1),u[23]||(u[23]=t("span",{class:"config-separator"},"•",-1)),void 0!==e.parameters.temperature?(a(),n("span",Ye," T="+i(e.parameters.temperature),1)):s("",!0),void 0!==e.parameters.max_tokens?(a(),n("span",Ge," Max="+i(e.parameters.max_tokens),1)):s("",!0)]),t("div",Je,[t("span",He,[u[24]||(u[24]=d(" Total: ")),t("strong",null,"$"+i(((un(e)||0)*re.value).toFixed(2)),1)]),o(A,{type:"text",size:"small",onClick:e=>function(e){const n=Y.value[e],a=O.getModel(n.provider,n.modelId);if(a){L.value=a,z.value=P.flattenParameters(n.parameters);const e=N.getResponseModes(n.provider);K.value="text";for(const[a,t]of Object.entries(e)){const e=t.parameters||{};if(Object.keys(e).every(e=>e in n.parameters)&&Object.keys(e).length>0){K.value=a;break}}}mn(e)}(r)},{default:c(()=>u[25]||(u[25]=[d("Edit")])),_:2,__:[25]},1032,["onClick"]),o(A,{type:"text",size:"small",danger:"",onClick:e=>mn(r)},{default:c(()=>u[26]||(u[26]=[d("Remove")])),_:2,__:[26]},1032,["onClick"])])]))),128))])),Y.value.length>0?(a(),n("div",Ve,[t("h3",null,[o(m(D)),u[28]||(u[28]=d(" Repeat Configuration ")),o(T,{title:"Test consistency by running each prompt multiple times"},{default:c(()=>[o(m(q),{style:{"margin-left":"8px",color:"#999","font-size":"14px"}})]),_:1}),o(R,{color:"blue",style:{"margin-left":"8px"}},{default:c(()=>u[27]||(u[27]=[d("New")])),_:1,__:[27]})]),o(w,{layout:"vertical",size:"large"},{default:c(()=>[o(C,{gutter:16,align:"middle"},{default:c(()=>[o(x,{span:8},{default:c(()=>[o(k,{label:"Calls per prompt",style:{"margin-bottom":"0"}},{default:c(()=>[o(_,{value:G.value.callsPerPrompt,"onUpdate:value":u[4]||(u[4]=e=>G.value.callsPerPrompt=e),min:1,max:10,style:{width:"100%"}},null,8,["value"])]),_:1})]),_:1}),G.value.callsPerPrompt>1?(a(),y(x,{key:0,span:8},{default:c(()=>[o(k,{label:"Delay between calls (ms)",style:{"margin-bottom":"0"}},{default:c(()=>[o(_,{value:G.value.delayBetweenRepeats,"onUpdate:value":u[5]||(u[5]=e=>G.value.delayBetweenRepeats=e),min:0,max:5e3,step:100,style:{width:"100%"},placeholder:"0"},null,8,["value"])]),_:1})]),_:1})):s("",!0),G.value.callsPerPrompt>1?(a(),y(x,{key:1,span:8},{default:c(()=>[o(k,{label:"Aggregation",style:{"margin-bottom":"0"}},{default:c(()=>[o(v,{value:G.value.aggregation,"onUpdate:value":u[6]||(u[6]=e=>G.value.aggregation=e),style:{width:"100%"}},{default:c(()=>[o(f,{value:"all"},{default:c(()=>u[29]||(u[29]=[d("Keep all responses")])),_:1,__:[29]}),o(f,{value:"average"},{default:c(()=>u[30]||(u[30]=[d("Average (numeric only)")])),_:1,__:[30]}),o(f,{value:"majority"},{default:c(()=>u[31]||(u[31]=[d("Majority vote")])),_:1,__:[31]})]),_:1},8,["value"])]),_:1})]),_:1})):s("",!0)]),_:1})]),_:1}),G.value.callsPerPrompt>1?(a(),y(M,{key:0,type:"info","show-icon":"",style:{"margin-top":"12px"}},{message:c(()=>[d(" Total API calls will be "+i(oe.value)+" ("+i(Y.value.length)+" models × "+i(re.value)+" prompts × "+i(G.value.callsPerPrompt)+" repeats) ",1)]),_:1})):s("",!0)])):s("",!0),Y.value.length>0&&W.value>0?(a(),n("div",We,[t("div",Xe,[t("div",Qe,[u[32]||(u[32]=t("span",{class:"summary-label"},"Variables:",-1)),t("strong",null,i(re.value),1)]),t("div",Ze,[u[33]||(u[33]=t("span",{class:"summary-label"},"Configs:",-1)),t("strong",null,i(Y.value.length),1)]),t("div",en,[u[34]||(u[34]=t("span",{class:"summary-label"},"Total Calls:",-1)),t("strong",nn,[d(i(oe.value)+" ",1),G.value.callsPerPrompt>1?(a(),n("span",an," (×"+i(G.value.callsPerPrompt)+") ",1)):s("",!0)])]),t("div",tn,[u[35]||(u[35]=t("span",{class:"summary-label"},"Cost:",-1)),t("strong",sn,"$"+i(pe.value.toFixed(2)),1)])])])):s("",!0)])]),o(ce,null,{default:c(()=>[o(A,{onClick:u[7]||(u[7]=n=>e.$emit("close")),size:"large"},{default:c(()=>u[37]||(u[37]=[d(" Cancel ")])),_:1,__:[37]}),o(A,{type:"primary",onClick:gn,disabled:!rn.value,size:"large"},{default:c(()=>u[38]||(u[38]=[d(" Create Trial ")])),_:1,__:[38]},8,["disabled"])]),_:1})])]),_:1})}}}),[["__scopeId","data-v-7812659b"]]),ln={class:"trial-overview"},on={class:"overview-section"},pn={class:"cost-value"},cn={key:0,class:"text-secondary",style:{"margin-left":"8px"}},dn={class:"overview-section"},mn={class:"progress-details"},un={class:"progress-stats"},fn={class:"configurations-section"},vn={key:0,class:"model-info"},gn={key:0},_n={class:"prompt-section"},yn={key:0,class:"variables-section"},hn=e({__name:"TrialDetailModal",props:{trial:{}},emits:["close","updated"],setup(e){const l=e,m=[{title:"Model",key:"model",width:200},{title:"Parameters",key:"params",width:300},{title:"Cost/Call",key:"cost",width:100,align:"right"}],u=v(()=>0===l.trial.progress.total?0:Math.round(l.trial.progress.completed/l.trial.progress.total*100));function f(e){const n=Object.entries(e).map(([e,n])=>`${e}: ${n}`).join(", ");return n.length>50?n.substring(0,50)+"...":n}function g(){const e=l.trial.variableSnapshots;return e&&0!==e.length?e.map(e=>({variable:e.variableName,listName:e.originalListName,count:e.data.itemCount})):[]}return(e,l)=>{const v=p("a-button"),_=p("a-tag"),h=p("a-descriptions-item"),b=p("a-descriptions"),k=p("a-progress"),x=p("a-typography-text"),P=p("a-table"),C=p("a-typography-paragraph"),w=p("a-list-item-meta"),I=p("a-list-item"),S=p("a-list");return a(),y(K,{"model-value":!0,title:e.trial.name,size:"full","onUpdate:modelValue":l[1]||(l[1]=n=>e.$emit("close"))},{footer:c(()=>[o(v,{onClick:l[0]||(l[0]=n=>e.$emit("close")),size:"large"},{default:c(()=>l[2]||(l[2]=[d(" Close ")])),_:1,__:[2]})]),default:c(()=>[t("div",ln,[t("div",on,[l[3]||(l[3]=t("h3",null,"Trial Information",-1)),o(b,{column:2,size:"small",bordered:""},{default:c(()=>[o(h,{label:"Status"},{default:c(()=>{return[o(_,{color:(n=e.trial.status,{completed:"success",failed:"error",running:"processing",cancelled:"default",draft:"default",pending:"processing",paused:"warning"}[n]||"default")},{default:c(()=>[d(i(e.trial.status.toUpperCase()),1)]),_:1},8,["color"])];var n}),_:1}),o(h,{label:"Design"},{default:c(()=>[d(i(e.trial.designSnapshot.originalName),1)]),_:1}),o(h,{label:"Created"},{default:c(()=>{return[d(i((n=e.trial.created,new Date(n).toLocaleString())),1)];var n}),_:1}),o(h,{label:"Estimated Cost"},{default:c(()=>[t("span",pn,"$"+i(e.trial.estimatedCost.toFixed(3)),1)]),_:1}),e.trial.repeatConfig?.callsPerPrompt&&e.trial.repeatConfig.callsPerPrompt>1?(a(),y(h,{key:0,label:"Repeat Configuration"},{default:c(()=>[o(_,{color:"purple"},{default:c(()=>[d(i(e.trial.repeatConfig.callsPerPrompt)+"× repeat",1)]),_:1}),e.trial.repeatConfig.delayBetweenRepeats?(a(),n("span",cn,i(e.trial.repeatConfig.delayBetweenRepeats)+"ms delay ",1)):s("",!0)]),_:1})):s("",!0)]),_:1})]),t("div",dn,[l[4]||(l[4]=t("h3",null,"Progress",-1)),t("div",mn,[o(k,{percent:u.value,status:"failed"===e.trial.status?"exception":"active",size:"small"},null,8,["percent","status"]),t("div",un,[o(_,null,{default:c(()=>[d(i(e.trial.progress.completed)+" / "+i(e.trial.progress.total)+" completed",1)]),_:1}),e.trial.progress.networkErrors>0?(a(),y(_,{key:0,color:"error"},{default:c(()=>[d(i(e.trial.progress.networkErrors)+" network errors ",1)]),_:1})):s("",!0)])])])]),t("div",fn,[t("h3",null,"Configurations ("+i(e.trial.configurationSnapshots.length)+")",1),o(P,{columns:m,"data-source":e.trial.configurationSnapshots,pagination:!1,size:"small",scroll:{y:300},"row-key":"id"},{bodyCell:c(({column:e,record:l})=>["model"===e.key?(a(),n("div",vn,[t("strong",null,i(l.provider),1),t("small",null,i(l.modelId),1)])):s("",!0),"params"===e.key?(a(),y(x,{key:1,code:"",class:"params-preview"},{default:c(()=>[d(i(f(l.parameters)),1)]),_:2},1024)):s("",!0),"cost"===e.key?(a(),n(r,{key:2},[(a(),n("span",gn," $"+i(.001.toFixed(4)),1))],64)):s("",!0)]),_:1},8,["data-source"])]),t("div",_n,[l[5]||(l[5]=t("h3",null,"Prompt Template",-1)),o(C,{code:"",class:"prompt-template"},{default:c(()=>[d(i(e.trial.designSnapshot.promptTemplate),1)]),_:1})]),e.trial.variableSnapshots?.length?(a(),n("div",yn,[l[6]||(l[6]=t("h3",null,"Variables",-1)),o(S,{"data-source":g(),size:"small",split:!1},{renderItem:c(({item:e})=>[o(I,null,{default:c(()=>[o(w,null,{title:c(()=>[o(_,{color:"blue"},{default:c(()=>[d(i(e.variable),1)]),_:2},1024)]),description:c(()=>[t("span",null,i(e.listName)+" ("+i(e.count)+" values)",1)]),_:2},1024)]),_:2},1024)]),_:1},8,["data-source"])])):s("",!0)]),_:1},8,["title"])}}});class bn{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=new E({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e),a=this.extractUniqueVariables(n),t=e.configurationSnapshots.map(e=>({provider:e.provider,modelId:e.modelId,displayName:e.name,parameters:e.parameters})),s=new Set(t.map(e=>e.provider)),r={};for(const l of s){const e=N.getProvider(l);e&&(r[l]=this.buildProviderConfig(l,e))}return{experiment:{promptTemplate:e.designSnapshot.promptTemplate,variables:a},models:t,providerConfigs:r}}static extractUniqueVariables(e){const n={};for(const t of e)for(const[e,a]of Object.entries(t.variables))n[e]||(n[e]=new Set),n[e].add(a);const a={};for(const[t,s]of Object.entries(n))a[t]=Array.from(s).sort();return a}static buildProviderConfig(e,n){const a=n.requestTransform||{},t=n.auth||{type:"none"};let s="direct";"messages"===a.promptKey&&a.wrapPrompt?s="messages":"input"===a.promptKey&&(s="input");let r,l,i="root";"ollama-chat"===e?(i="options",r={max_tokens:"num_predict",max_completion_tokens:"num_predict"}):"ollama-generate"===e&&(i="mixed",l={root:["model","prompt","stream","format","raw"],options:["temperature","num_predict","top_k","top_p"]},r={max_tokens:"num_predict",max_completion_tokens:"num_predict"});const o=Object.values(n.responseModes||{})[0],p=this.parseResponsePath(o?.responseTransform?.contentPath),c=o?.responseTransform?.fallbackPaths?.map(e=>this.parseResponsePath(e)),d=n.api.baseUrl+(n.api.endpoints.chat||n.api.endpoints.generate||"");return{name:n.name,endpoint:d,auth:{type:t.type,header:t.header,prefix:"bearer"===t.type?"Bearer":void 0},headers:n.headers,request:{modelPrefixStrip:!0,promptFormat:s,messageRole:a.messageRole,paramLocation:i,paramRenames:r,mixedParams:l},response:{successPath:p,fallbackPaths:c,errorPath:["error","message"]}}}static parseResponsePath(e){return e?e.split(/[\.\[\]]/).filter(Boolean).map(e=>{const n=parseInt(e);return isNaN(n)?e:n}):["content"]}static generateScript(e,n){const a=(new Date).toISOString(),t=JSON.stringify(e.experiment.variables,null,4),s=JSON.stringify(e.models,null,4),r=JSON.stringify(e.providerConfigs,null,4);return`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Simple Mode\n=====================================\nGenerated by Auditomatic Lite v${L.short} on ${a}\n\nThis script reproduces your experiment by generating API calls from variables.\nPerfect for understanding, modifying, and extending your experiments.\n\nOriginal trial: ${n}\n"""\n\nimport os\nimport json\nimport time\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\nAPI_KEYS = {\n${Object.keys(e.providerConfigs).map(e=>{const n=e.split("-")[0].toUpperCase();return`    "${e}": os.environ.get("${n}_API_KEY", ""),`}).join("\n")}\n}\n\n# Your experiment design\nEXPERIMENT = {\n    "prompt_template": "${e.experiment.promptTemplate.replace(/"/g,'\\"')}",\n    "variables": ${t}\n}\n\n# Models to test\nMODELS = ${s}\n\n# Provider configurations (how to talk to each API)\nPROVIDER_CONFIGS = ${r}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\ndef make_api_call(provider_id: str, model: str, prompt: str, params: dict) -> dict:\n    """\n    Universal API caller that handles all provider quirks.\n    \n    Returns dict with 'success', 'content', 'error', and timing info.\n    """\n    config = PROVIDER_CONFIGS[provider_id]\n    \n    # Build headers\n    headers = {"Content-Type": "application/json"}\n    \n    # Add authentication\n    auth = config["auth"]\n    if auth["type"] == "bearer":\n        api_key = API_KEYS.get(provider_id, "")\n        if not api_key:\n            return {"success": False, "error": f"No API key for {provider_id}"}\n        headers[auth["header"]] = f"{auth['prefix']} {api_key}"\n    elif auth["type"] == "header":\n        api_key = API_KEYS.get(provider_id, "")\n        if not api_key:\n            return {"success": False, "error": f"No API key for {provider_id}"}\n        headers[auth["header"]] = api_key\n    \n    # Add provider-specific headers\n    if config.get("headers"):\n        headers.update(config["headers"])\n    \n    # Build request body\n    request = config["request"]\n    \n    # Strip provider prefix from model\n    if request.get("modelPrefixStrip"):\n        model = model.split(":", 1)[-1]\n    \n    body = {"model": model}\n    \n    # Format prompt\n    if request["promptFormat"] == "messages":\n        body["messages"] = [{"role": request.get("messageRole", "user"), "content": prompt}]\n    elif request["promptFormat"] == "direct":\n        body["prompt"] = prompt\n    elif request["promptFormat"] == "input":\n        body["input"] = prompt\n    \n    # Handle parameters\n    processed_params = params.copy()\n    \n    # Apply renames\n    if request.get("paramRenames"):\n        for old_key, new_key in request["paramRenames"].items():\n            if old_key in processed_params:\n                processed_params[new_key] = processed_params.pop(old_key)\n    \n    # Place parameters\n    if request["paramLocation"] == "root":\n        body.update(processed_params)\n    elif request["paramLocation"] == "options":\n        body["options"] = processed_params\n    elif request.get("mixedParams"):\n        mixed = request["mixedParams"]\n        for key, value in processed_params.items():\n            if key in mixed.get("root", []):\n                body[key] = value\n            else:\n                if "options" not in body:\n                    body["options"] = {}\n                body["options"][key] = value\n    \n    # Make request\n    start_time = time.time()\n    try:\n        response = requests.post(\n            config["endpoint"],\n            headers=headers,\n            json=body,\n            timeout=30\n        )\n        latency_ms = (time.time() - start_time) * 1000\n        \n        if response.ok:\n            data = response.json()\n            content = extract_from_path(data, config["response"]["successPath"])\n            \n            # Try fallback paths\n            if content is None and config["response"].get("fallbackPaths"):\n                for path in config["response"]["fallbackPaths"]:\n                    content = extract_from_path(data, path)\n                    if content is not None:\n                        break\n            \n            return {\n                "success": True,\n                "content": content or "",\n                "latency_ms": latency_ms,\n                "status_code": response.status_code\n            }\n        else:\n            return {\n                "success": False,\n                "error": f"HTTP {response.status_code}: {response.text[:200]}",\n                "latency_ms": latency_ms,\n                "status_code": response.status_code\n            }\n            \n    except Exception as e:\n        return {\n            "success": False,\n            "error": str(e),\n            "latency_ms": (time.time() - start_time) * 1000\n        }\n\ndef extract_from_path(data: Any, path: List[Any]) -> Optional[str]:\n    """Extract value from nested data using a path like ['choices', 0, 'message', 'content']"""\n    try:\n        current = data\n        for key in path:\n            if isinstance(current, dict):\n                current = current[key]\n            elif isinstance(current, list):\n                current = current[int(key)]\n            else:\n                return None\n        return str(current) if current is not None else None\n    except (KeyError, IndexError, TypeError):\n        return None\n\ndef generate_prompts():\n    """Generate all prompts from template and variables"""\n    template = EXPERIMENT["prompt_template"]\n    variables = EXPERIMENT["variables"]\n    \n    # Get variable names from template\n    import re\n    var_names = re.findall(r'{{(\\w+)}}', template)\n    \n    # Generate all combinations\n    from itertools import product\n    \n    var_lists = [variables[var] for var in var_names]\n    for values in product(*var_lists):\n        var_dict = dict(zip(var_names, values))\n        \n        # Replace variables in template\n        prompt = template\n        for var, val in var_dict.items():\n            prompt = prompt.replace(f"{{{{{var}}}}}", str(val))\n        \n        yield prompt, var_dict\n\ndef run_experiment():\n    """Run the full experiment"""\n    results = []\n    total_calls = len(MODELS) * len(list(generate_prompts()))\n    current = 0\n    \n    print(f"Running experiment with {len(MODELS)} models and {total_calls} total API calls")\n    print("=" * 60)\n    \n    for model_config in MODELS:\n        print(f"\\nTesting {model_config['displayName']}...")\n        \n        for prompt, variables in generate_prompts():\n            current += 1\n            print(f"[{current}/{total_calls}] {prompt[:50]}...", end=" ")\n            \n            # Make API call\n            result = make_api_call(\n                model_config["provider"],\n                model_config["modelId"],\n                prompt,\n                model_config["parameters"]\n            )\n            \n            # Collect results\n            results.append({\n                "timestamp": datetime.now(),\n                "provider": model_config["provider"],\n                "model": model_config["modelId"],\n                "model_name": model_config["displayName"],\n                "prompt": prompt,\n                "response": result.get("content", ""),\n                "success": result.get("success", False),\n                "error": result.get("error", ""),\n                "latency_ms": result.get("latency_ms", 0),\n                "status_code": result.get("status_code", 0),\n                **variables  # Add variables as columns\n            })\n            \n            # Show result\n            if result["success"]:\n                print(f"✓ {result['content'][:30]}")\n            else:\n                print(f"✗ {result['error'][:30]}")\n            \n            # Rate limiting\n            time.sleep(0.1)\n    \n    return results\n\ndef save_results(results: List[Dict[str, Any]], format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_results_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for API keys\n    missing_keys = []\n    for model in MODELS:\n        provider = model["provider"]\n        if provider not in API_KEYS or not API_KEYS[provider]:\n            missing_keys.append(provider)\n    \n    if missing_keys:\n        print("WARNING: Missing API keys for:", ", ".join(set(missing_keys)))\n        print("Set them in the API_KEYS dict or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Run experiment\n    results = run_experiment()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if 'latency_ms' in df.columns:\n            print(f"Avg latency: {df['latency_ms'].mean():.1f}ms")\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class kn{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=[],a=new w,t=new E({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e),s=O(e);let r=0;for(const i of e.configurationSnapshots){const o=N.getProvider(i.provider);if(o)for(const p of t){const t=s>1?T():void 0;let c=e.designSnapshot.promptTemplate;for(const[e,n]of Object.entries(p.variables))c=c.replace(new RegExp(`{{${e}}}`,"g"),n);for(let e=0;e<s;e++){r++;try{const l={id:"export-config",name:i.name,provider:i.provider,model:i.modelId,params:i.parameters,created_at:new Date},d=a.buildAPIRequest(l,c),m={};for(const[e,n]of Object.entries(d.headers))"Authorization"===e&&n.startsWith("Bearer ")?m[e]=`Bearer $${i.provider.split("-")[0].toUpperCase()}_API_KEY`:e===o.auth.header&&"header"===o.auth.type?m[e]=`$${i.provider.split("-")[0].toUpperCase()}_API_KEY`:m[e]=n;const u=this.parseResponsePath(this.getDefaultResponsePath(i.provider));n.push({id:`call_${String(r).padStart(3,"0")}`,provider:i.provider,endpoint:d.url,headers:m,body:d.body,responsePath:u,metadata:{variables:p.variables,modelName:i.modelId,configName:i.name,...s>1&&{repeatIndex:e,repeatGroupId:t}}})}catch(l){console.warn(`Failed to build API call for ${i.name}:`,l)}}}}return{apiCalls:n,...e.repeatConfig&&{repeatConfig:{callsPerPrompt:e.repeatConfig.callsPerPrompt,delayBetweenRepeats:e.repeatConfig.delayBetweenRepeats}}}}static parseResponsePath(e){return e.split(/[\.\[\]]/).filter(Boolean).map(e=>{const n=parseInt(e);return isNaN(n)?e:n})}static getDefaultResponsePath(e){switch(e){case"openai-chat":case"openrouter":return"choices[0].message.content";case"openai-responses":return"output[0].content[0].text";case"anthropic":return"content[0].text";case"ollama-chat":return"message.content";case"ollama-generate":return"response";default:return"content"}}static generateScript(e,n){const a=(new Date).toISOString(),t=JSON.stringify(e.apiCalls,null,4),s=[...new Set(e.apiCalls.map(e=>e.provider))],r=e.repeatConfig?`\nRepeat configuration: ${e.repeatConfig.callsPerPrompt} calls per prompt${e.repeatConfig.delayBetweenRepeats?`, ${e.repeatConfig.delayBetweenRepeats}ms delay`:""}`:"";return`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Literal Mode\n======================================\nGenerated by Auditomatic Lite v${L.short} on ${a}\n\nThis script contains the EXACT API calls from your experiment.\nPerfect for bit-for-bit reproduction, debugging, and comparing results.\n\nOriginal trial: ${n}\nTotal API calls: ${e.apiCalls.length}${r}\n"""\n\nimport os\nimport json\nimport time\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\nAPI_KEYS = {\n${s.map(e=>{const n=e.split("-")[0].toUpperCase();return`    "${n}": os.environ.get("${n}_API_KEY", ""),`}).join("\n")}\n}\n\n# Pre-computed API calls from your experiment\nAPI_CALLS = ${t}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\ndef execute_literal_calls():\n    """Execute pre-serialized API calls exactly as specified"""\n    results = []\n    total = len(API_CALLS)\n    \n    print(f"Executing {total} pre-computed API calls...")\n    print("=" * 60)\n    \n    for i, call in enumerate(API_CALLS):\n        print(f"[{i+1}/{total}] {call['metadata']['configName']} - ", end="")\n        \n        # Replace API key placeholders in headers\n        headers = {}\n        for key, value in call["headers"].items():\n            if "\\$" in str(value):\n                # Extract provider name from placeholder\n                for provider_key, api_key in API_KEYS.items():\n                    placeholder = f"\\\${provider_key}_API_KEY"\n                    if placeholder in value:\n                        headers[key] = value.replace(placeholder, api_key)\n                        break\n                else:\n                    headers[key] = value\n            else:\n                headers[key] = value\n        \n        # Check if we have required API key\n        provider_base = call["provider"].split("-")[0].upper()\n        if provider_base in ["OPENAI", "ANTHROPIC", "OPENROUTER"] and not API_KEYS.get(provider_base):\n            results.append({\n                "call_id": call["id"],\n                "timestamp": datetime.now(),\n                "provider": call["provider"],\n                "model": call["metadata"]["modelName"],\n                "config_name": call["metadata"]["configName"],\n                "prompt": extract_prompt_from_body(call["body"]),\n                "response": "",\n                "success": False,\n                "error": f"No API key for {provider_base}",\n                "latency_ms": 0,\n                "status_code": 0,\n                **call["metadata"]["variables"]\n            })\n            print(f"✗ No API key")\n            continue\n        \n        # Make the exact API call\n        start_time = time.time()\n        try:\n            response = requests.post(\n                call["endpoint"],\n                headers=headers,\n                json=call["body"],\n                timeout=30\n            )\n            latency_ms = (time.time() - start_time) * 1000\n            \n            if response.ok:\n                data = response.json()\n                content = extract_from_path(data, call["responsePath"])\n                \n                results.append({\n                    "call_id": call["id"],\n                    "timestamp": datetime.now(),\n                    "provider": call["provider"],\n                    "model": call["metadata"]["modelName"],\n                    "config_name": call["metadata"]["configName"],\n                    "prompt": extract_prompt_from_body(call["body"]),\n                    "response": content or "",\n                    "success": True,\n                    "error": "",\n                    "latency_ms": latency_ms,\n                    "status_code": response.status_code,\n                    "full_response": json.dumps(data)[:500],  # First 500 chars\n                    **call["metadata"]["variables"]\n                })\n                print(f"✓ {(content or '')[:30]}")\n            else:\n                results.append({\n                    "call_id": call["id"],\n                    "timestamp": datetime.now(),\n                    "provider": call["provider"],\n                    "model": call["metadata"]["modelName"],\n                    "config_name": call["metadata"]["configName"],\n                    "prompt": extract_prompt_from_body(call["body"]),\n                    "response": "",\n                    "success": False,\n                    "error": f"HTTP {response.status_code}: {response.text[:200]}",\n                    "latency_ms": latency_ms,\n                    "status_code": response.status_code,\n                    **call["metadata"]["variables"]\n                })\n                print(f"✗ HTTP {response.status_code}")\n                \n        except Exception as e:\n            latency_ms = (time.time() - start_time) * 1000\n            results.append({\n                "call_id": call["id"],\n                "timestamp": datetime.now(),\n                "provider": call["provider"],\n                "model": call["metadata"]["modelName"],\n                "config_name": call["metadata"]["configName"],\n                "prompt": extract_prompt_from_body(call["body"]),\n                "response": "",\n                "success": False,\n                "error": str(e)[:200],\n                "latency_ms": latency_ms,\n                "status_code": 0,\n                **call["metadata"]["variables"]\n            })\n            print(f"✗ {str(e)[:30]}")\n        \n        # Handle repeat delays if configured\n        if "repeatIndex" in call["metadata"] and call["metadata"]["repeatIndex"] > 0:\n            # Check if there's a repeat delay configured\n            delay_ms = ${e.repeatConfig?.delayBetweenRepeats||0}\n            if delay_ms > 0:\n                time.sleep(delay_ms / 1000.0)\n        \n        # Rate limiting\n        time.sleep(0.1)\n    \n    return results\n\ndef extract_prompt_from_body(body: dict) -> str:\n    """Extract the prompt from various request body formats"""\n    # Messages format (OpenAI, Anthropic, etc)\n    if "messages" in body and isinstance(body["messages"], list):\n        for msg in body["messages"]:\n            if msg.get("role") == "user":\n                return msg.get("content", "")\n    \n    # Direct prompt format (Ollama generate)\n    if "prompt" in body:\n        return body["prompt"]\n    \n    # Input format (OpenAI responses)\n    if "input" in body:\n        return body["input"]\n    \n    return ""\n\ndef extract_from_path(data: Any, path: List[Any]) -> Optional[str]:\n    """Extract value from nested data using a path like ['choices', 0, 'message', 'content']"""\n    try:\n        current = data\n        for key in path:\n            if isinstance(current, dict):\n                current = current[key]\n            elif isinstance(current, list):\n                current = current[int(key)]\n            else:\n                return None\n        return str(current) if current is not None else None\n    except (KeyError, IndexError, TypeError):\n        return None\n\ndef save_results(results: List[Dict[str, Any]], format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    # Drop full_response column for cleaner output (except JSON)\n    if format != "json" and "full_response" in df.columns:\n        df = df.drop(columns=["full_response"])\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_literal_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for API keys\n    required_providers = set(call["provider"].split("-")[0].upper() for call in API_CALLS)\n    missing_keys = []\n    for provider in required_providers:\n        if provider not in ["OLLAMA"] and not API_KEYS.get(provider):\n            missing_keys.append(provider)\n    \n    if missing_keys:\n        print("WARNING: Missing API keys for:", ", ".join(missing_keys))\n        print("Set them in the API_KEYS dict or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Execute all calls\n    results = execute_literal_calls()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if df['success'].any():\n            print(f"Avg latency (successful): {df[df['success']]['latency_ms'].mean():.1f}ms")\n        \n        # Group by model\n        print(f"\\nBy Model:")\n        model_summary = df.groupby('config_name')['success'].agg(['count', 'sum', 'mean'])\n        model_summary.columns = ['total', 'successful', 'success_rate']\n        print(model_summary)\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class xn{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=new E({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e),a=this.extractUniqueVariables(n),t=e.configurationSnapshots.map(e=>{let n,a="text";return e.parameters.response_format?(a="json_mode",n={response_format:e.parameters.response_format}):e.parameters.tools&&(a="function_calling",n={tools:e.parameters.tools,tool_choice:e.parameters.tool_choice}),{provider:e.provider,modelId:e.modelId,displayName:e.name,parameters:this.filterCoreParams(e.parameters),responseMode:a,responseModeParams:n}}),s=new Set(t.map(e=>e.provider)),r={"openai-chat":"openai","openai-responses":"openai",anthropic:"anthropic",openrouter:"openai","ollama-chat":"ollama","ollama-generate":"ollama"},l=[...new Set(Array.from(s).map(e=>r[e]).filter(Boolean))],i={"openai-chat":"OPENAI","openai-responses":"OPENAI",anthropic:"ANTHROPIC",openrouter:"OPENROUTER","ollama-chat":"","ollama-generate":""},o=[...new Set(Array.from(s).map(e=>i[e]).filter(Boolean))];return{experiment:{promptTemplate:e.designSnapshot.promptTemplate,variables:a},models:t,providerLibraries:{required:l,apiKeys:o}}}static extractUniqueVariables(e){const n={};for(const t of e)for(const[e,a]of Object.entries(t.variables))n[e]||(n[e]=new Set),n[e].add(a);const a={};for(const[t,s]of Object.entries(n))a[t]=Array.from(s).sort();return a}static filterCoreParams(e){const n={...e};return delete n.response_format,delete n.tools,delete n.tool_choice,n}static generateScript(e,n){const a=(new Date).toISOString(),t=JSON.stringify(e.experiment.variables,null,4),s=JSON.stringify(e.models,null,4),r=["import os","import json","import time","import pandas as pd","from datetime import datetime"];return e.providerLibraries.required.includes("openai")&&r.push("from openai import OpenAI"),e.providerLibraries.required.includes("anthropic")&&r.push("from anthropic import Anthropic"),e.providerLibraries.required.includes("ollama")&&r.push("import ollama"),`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Native Mode\n=====================================\nGenerated by Auditomatic Lite v${L.short} on ${a}\n\nThis script uses native Python libraries for each provider.\nCleanest code, best for production use.\n\nOriginal trial: ${n}\nRequired packages: ${e.providerLibraries.required.join(", ")}\n"""\n\n${r.join("\n")}\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\n${e.providerLibraries.apiKeys.map(e=>`os.environ.setdefault("${e}_API_KEY", "")  # Set your ${e} API key`).join("\n")}\n\n# Your experiment design\nEXPERIMENT = {\n    "prompt_template": "${e.experiment.promptTemplate.replace(/"/g,'\\"')}",\n    "variables": ${t}\n}\n\n# Models to test\nMODELS = ${s}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\n# Initialize clients\nclients = {}\n\ndef get_client(provider):\n    """Get or create client for provider"""\n    if provider not in clients:\n        if provider in ["openai-chat", "openai-responses"]:\n            clients[provider] = OpenAI()\n        elif provider == "anthropic":\n            clients[provider] = Anthropic()\n        elif provider == "openrouter":\n            clients[provider] = OpenAI(\n                api_key=os.environ.get("OPENROUTER_API_KEY"),\n                base_url="https://openrouter.ai/api/v1"\n            )\n        # Ollama doesn't need a client\n    return clients.get(provider)\n\ndef make_api_call(model_config: dict, prompt: str) -> dict:\n    """Make API call using native provider library"""\n    provider = model_config["provider"]\n    model = model_config["modelId"]\n    params = model_config["parameters"].copy()\n    \n    try:\n        start_time = time.time()\n        \n        if provider == "openai-chat" or provider == "openrouter":\n            client = get_client(provider)\n            \n            # Build messages\n            messages = [{"role": "user", "content": prompt}]\n            \n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["response_format"] = {"type": "json_object"}\n            elif model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                **params\n            )\n            \n            # Extract content based on response mode\n            if model_config["responseMode"] == "function_calling" and response.choices[0].message.tool_calls:\n                content = response.choices[0].message.tool_calls[0].function.arguments\n                if isinstance(content, str):\n                    content = json.loads(content)\n            else:\n                content = response.choices[0].message.content\n            \n        elif provider == "openai-responses":\n            client = get_client(provider)\n            \n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["text"] = {"format": {"type": "json_object"}}\n            elif model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.responses.create(\n                model=model,\n                input=prompt,\n                **params\n            )\n            \n            # Extract content\n            output = response.output\n            if isinstance(output, list) and len(output) > 0:\n                if hasattr(output[0], 'content') and isinstance(output[0].content, list):\n                    content = output[0].content[0].text if hasattr(output[0].content[0], 'text') else str(output[0].content[0])\n                else:\n                    content = str(output[0])\n            else:\n                content = str(output)\n            \n        elif provider == "anthropic":\n            client = get_client(provider)\n            \n            # Build messages\n            messages = [{"role": "user", "content": prompt}]\n            \n            # Handle response modes\n            if model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.messages.create(\n                model=model,\n                messages=messages,\n                **params\n            )\n            \n            # Extract content\n            if model_config["responseMode"] == "function_calling" and hasattr(response.content[0], 'input'):\n                content = response.content[0].input\n            else:\n                content = response.content[0].text\n            \n        elif provider == "ollama-chat":\n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["format"] = "json"\n            \n            # Make call\n            response = ollama.chat(\n                model=model,\n                messages=[{"role": "user", "content": prompt}],\n                **params\n            )\n            \n            # Extract content\n            content = response["message"]["content"]\n            \n        elif provider == "ollama-generate":\n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["format"] = "json"\n            \n            # Make call\n            response = ollama.generate(\n                model=model,\n                prompt=prompt,\n                **params\n            )\n            \n            # Extract content\n            content = response["response"]\n        \n        else:\n            raise ValueError(f"Unknown provider: {provider}")\n        \n        latency_ms = (time.time() - start_time) * 1000\n        \n        return {\n            "success": True,\n            "content": content,\n            "latency_ms": latency_ms\n        }\n        \n    except Exception as e:\n        latency_ms = (time.time() - start_time) * 1000\n        return {\n            "success": False,\n            "content": "",\n            "error": str(e),\n            "latency_ms": latency_ms\n        }\n\ndef generate_prompts():\n    """Generate all prompts from template and variables"""\n    template = EXPERIMENT["prompt_template"]\n    variables = EXPERIMENT["variables"]\n    \n    # Get variable names from template\n    import re\n    var_names = re.findall(r'{{(\\w+)}}', template)\n    \n    # Generate all combinations\n    from itertools import product\n    \n    var_lists = [variables[var] for var in var_names]\n    for values in product(*var_lists):\n        var_dict = dict(zip(var_names, values))\n        \n        # Replace variables in template\n        prompt = template\n        for var, val in var_dict.items():\n            prompt = prompt.replace(f"{{{{{var}}}}}", str(val))\n        \n        yield prompt, var_dict\n\ndef run_experiment():\n    """Run the full experiment"""\n    results = []\n    total_calls = len(MODELS) * len(list(generate_prompts()))\n    current = 0\n    \n    print(f"Running experiment with {len(MODELS)} models and {total_calls} total API calls")\n    print("=" * 60)\n    \n    for model_config in MODELS:\n        print(f"\\nTesting {model_config['displayName']}...")\n        \n        for prompt, variables in generate_prompts():\n            current += 1\n            print(f"[{current}/{total_calls}] {prompt[:50]}...", end=" ")\n            \n            # Make API call\n            result = make_api_call(model_config, prompt)\n            \n            # Collect results\n            results.append({\n                "timestamp": datetime.now(),\n                "provider": model_config["provider"],\n                "model": model_config["modelId"],\n                "model_name": model_config["displayName"],\n                "prompt": prompt,\n                "response": str(result.get("content", "")),\n                "success": result.get("success", False),\n                "error": result.get("error", ""),\n                "latency_ms": result.get("latency_ms", 0),\n                **variables  # Add variables as columns\n            })\n            \n            # Show result\n            if result["success"]:\n                print(f"✓ {str(result['content'])[:30]}")\n            else:\n                print(f"✗ {result['error'][:30]}")\n            \n            # Rate limiting\n            time.sleep(0.1)\n    \n    return results\n\ndef save_results(results: list, format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_native_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for required packages\n    required = ${JSON.stringify(e.providerLibraries.required)}\n    missing = []\n    for lib in required:\n        try:\n            __import__(lib)\n        except ImportError:\n            missing.append(lib)\n    \n    if missing:\n        print(f"ERROR: Missing required packages: {', '.join(missing)}")\n        print(f"Install with: pip install {' '.join(missing)}")\n        return\n    \n    # Check for API keys\n    missing_keys = []\n    for model in MODELS:\n        provider = model["provider"]\n        if provider in ["openai-chat", "openai-responses"] and not os.environ.get("OPENAI_API_KEY"):\n            missing_keys.append("OPENAI_API_KEY")\n        elif provider == "anthropic" and not os.environ.get("ANTHROPIC_API_KEY"):\n            missing_keys.append("ANTHROPIC_API_KEY")\n        elif provider == "openrouter" and not os.environ.get("OPENROUTER_API_KEY"):\n            missing_keys.append("OPENROUTER_API_KEY")\n    \n    if missing_keys:\n        print(f"WARNING: Missing API keys: {', '.join(set(missing_keys))}")\n        print("Set them in the script or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Run experiment\n    results = run_experiment()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if 'latency_ms' in df.columns and df['success'].any():\n            print(f"Avg latency: {df[df['success']]['latency_ms'].mean():.1f}ms")\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class Pn{static async generatePythonScript(e,n){try{const a=n||this.getDefaultOptions(),t=this.validateTrialForExport(e);if(!t.valid)throw new Error(`Trial validation failed: ${t.errors.join(", ")}`);switch(a.mode){case"simple":return bn.generate(e);case"literal":return kn.generate(e);case"native":return xn.generate(e);default:throw new Error(`Unknown export mode: ${a.mode}`)}}catch(a){throw new Error(`Failed to generate Python export: ${a instanceof Error?a.message:String(a)}`)}}static async downloadPythonScript(e,n){const a=await this.generatePythonScript(e,n),t=n||this.getDefaultOptions(),s=new Blob([a],{type:"text/x-python"}),r=URL.createObjectURL(s),l=document.createElement("a");l.href=r,l.download=this.generateFilename(e,t.mode),document.body.appendChild(l),l.click(),document.body.removeChild(l),URL.revokeObjectURL(r)}static validateTrialForExport(e){const n=[];return e.designSnapshot?e.designSnapshot.promptTemplate||n.push("Design missing prompt template"):n.push("Trial missing design snapshot"),e.configurationSnapshots&&0!==e.configurationSnapshots.length?e.configurationSnapshots.forEach((e,a)=>{e.provider||n.push(`Configuration ${a+1} missing provider`),e.modelId||n.push(`Configuration ${a+1} missing model`),e.parameters||n.push(`Configuration ${a+1} missing parameters`)}):n.push("Trial missing model configurations"),e.variableSnapshots||n.push("Trial missing variable snapshots"),{valid:0===n.length,errors:n}}static getExportSummary(e){const n=new Set(e.configurationSnapshots.map(e=>e.provider)),a=e.totalCombinations||0;return{apiCallCount:e.configurationSnapshots.length*a,providersUsed:Array.from(n),variableCombinations:a,configurations:e.configurationSnapshots.length}}static getDefaultOptions(){return{mode:"simple"}}static generateFilename(e,n){const a=e.name||`trial_${e.id}`,t=(new Date).toISOString().split("T")[0];return`${a.toLowerCase().replace(/[^a-z0-9]/g,"_")}_${n}_${t}.py`}}const Cn=Object.freeze(Object.defineProperty({__proto__:null,PythonExportService:Pn},Symbol.toStringTag,{value:"Module"})),wn={class:"trial-info"},In={class:"trial-stats"},Sn={class:"export-section"},An={class:"mode-content"},En={class:"mode-content"},On={class:"mode-content"},Tn={class:"export-section"},Nn={class:"preview-content"},Rn={class:"preview-info"},Mn=e({__name:"PythonExportModal",props:{trial:{}},emits:["close","exported"],setup(e,{emit:n}){const s=e,r=n,l=f("simple"),m=f(!1),u=v(()=>s.trial.progress.total),g=v(()=>s.trial.configurationSnapshots?.length||0),_=v(()=>s.trial.totalCombinations||0),h=v(()=>{const e=.05*_.value+.3*g.value;return Math.round(15+e)}),b=v(()=>{const e=.5*u.value;return Math.round(10+e)}),k=v(()=>{const e=.05*_.value+.2*g.value;return Math.round(12+e)}),x=v(()=>{const e=s.trial.name.toLowerCase().replace(/\s+/g,"_"),n=(new Date).toISOString().split("T")[0];return`${e}_${l.value}_${n}.py`}),P=v(()=>{if("simple"===l.value){return 300+(_.value+10*g.value)}if("native"===l.value){return 250+(_.value+8*g.value)}return 200+15*u.value});async function C(){m.value=!0;try{const e={mode:l.value};await Pn.downloadPythonScript(s.trial,e),r("exported",x.value),r("close")}catch(e){console.error("Export failed:",e),alert("Export failed: "+(e instanceof Error?e.message:"Unknown error"))}finally{m.value=!1}}return(e,n)=>{const s=p("a-button"),r=p("a-tag"),f=p("a-radio"),v=p("a-radio-group"),w=p("a-typography-text");return a(),y(K,{"model-value":!0,title:"Export Python Script",size:"full","onUpdate:modelValue":n[2]||(n[2]=n=>e.$emit("close"))},{footer:c(()=>[o(s,{onClick:n[0]||(n[0]=n=>e.$emit("close")),size:"large"},{default:c(()=>n[3]||(n[3]=[d(" Cancel ")])),_:1,__:[3]}),o(s,{type:"primary",onClick:C,loading:m.value,size:"large"},{default:c(()=>n[4]||(n[4]=[d(" Export Script ")])),_:1,__:[4]},8,["loading"])]),default:c(()=>[t("div",wn,[t("h3",null,i(e.trial.name),1),t("div",In,[o(r,null,{default:c(()=>[d(i(u.value)+" API calls",1)]),_:1}),o(r,null,{default:c(()=>[d(i(g.value)+" configurations",1)]),_:1}),o(r,null,{default:c(()=>[d(i(_.value)+" variable combinations",1)]),_:1})])]),t("div",Sn,[n[11]||(n[11]=t("h4",null,"Export Mode",-1)),o(v,{value:l.value,"onUpdate:value":n[1]||(n[1]=e=>l.value=e),class:"mode-options"},{default:c(()=>[o(f,{value:"simple",class:"mode-radio"},{default:c(()=>[t("div",An,[n[5]||(n[5]=t("div",{class:"mode-title"},"Simple Script",-1)),n[6]||(n[6]=t("div",{class:"mode-description"}," Educational script with variables as lists. Easy to understand, modify, and extend. Perfect for learning how AI APIs work. ",-1)),o(r,{color:"blue",size:"small"},{default:c(()=>[d("~"+i(h.value)+"KB",1)]),_:1})])]),_:1}),o(f,{value:"literal",class:"mode-radio"},{default:c(()=>[t("div",En,[n[7]||(n[7]=t("div",{class:"mode-title"},"Literal Reproduction",-1)),n[8]||(n[8]=t("div",{class:"mode-description"}," Exact API calls pre-computed. Bit-for-bit reproduction of your experiment. Best for debugging and comparing results. ",-1)),o(r,{color:"blue",size:"small"},{default:c(()=>[d("~"+i(b.value)+"KB",1)]),_:1})])]),_:1}),o(f,{value:"native",class:"mode-radio"},{default:c(()=>[t("div",On,[n[9]||(n[9]=t("div",{class:"mode-title"},"Native Libraries",-1)),n[10]||(n[10]=t("div",{class:"mode-description"}," Uses official Python SDKs (openai, anthropic, ollama). Cleanest code, best for production use. Requires: pip install openai anthropic ollama ",-1)),o(r,{color:"green",size:"small"},{default:c(()=>[d("~"+i(k.value)+"KB",1)]),_:1})])]),_:1})]),_:1},8,["value"])]),n[13]||(n[13]=t("div",{class:"export-section"},[t("h4",null,"Output Format"),t("div",{class:"format-info"},[t("p",null,"Both scripts save results using pandas in your choice of format:"),t("ul",null,[t("li",null,[t("strong",null,"CSV"),d(" - Universal format, opens in Excel/Google Sheets")]),t("li",null,[t("strong",null,"Excel"),d(" - Native Excel format")]),t("li",null,[t("strong",null,"JSON"),d(" - For programmatic access")]),t("li",null,[t("strong",null,"Parquet"),d(" - Efficient compressed format")]),t("li",null,[t("strong",null,"HTML"),d(" - For web viewing")]),t("li",null,[t("strong",null,"Markdown"),d(" - For documentation")]),t("li",null,[t("strong",null,"Stata"),d(" - For statistical analysis")]),t("li",null,[t("strong",null,"Pickle"),d(" - Python native format")])])])],-1)),t("div",Tn,[n[12]||(n[12]=t("h4",null,"Script Preview",-1)),t("div",Nn,[o(w,{code:"",class:"preview-filename"},{default:c(()=>[d(i(x.value),1)]),_:1}),t("div",Rn,[o(r,{size:"small"},{default:c(()=>[d(i(P.value)+" lines",1)]),_:1}),o(r,{size:"small"},{default:c(()=>[d(i(l.value)+" mode",1)]),_:1})])])])]),_:1,__:[13]})}}}),jn={class:"api-call-modal"},$n={class:"modal-header"},Fn={class:"modal-content"},Un={class:"section"},Dn={class:"info-grid"},qn={class:"info-item"},Ln={class:"call-id"},zn={class:"info-item"},Bn={class:"info-item"},Kn={class:"info-item"},Yn={key:0,class:"info-item"},Gn={key:1,class:"info-item"},Jn={key:2,class:"info-item"},Hn={class:"section"},Vn={class:"variables-detail"},Wn={class:"variable-value"},Xn={key:0,class:"attributes-section"},Qn={class:"attribute-items"},Zn={class:"section"},ea={class:"prompt-display"},na={key:0,class:"section"},aa={key:0,class:"response-info"},ta={class:"info-grid"},sa={class:"info-item"},ra={class:"info-item"},la={key:1,class:"result-content"},ia={key:0,class:"error-result"},oa={class:"error-message"},pa={key:0,class:"error-raw"},ca={class:"error-response"},da={key:1,class:"content-result"},ma={class:"content-display"},ua={class:"section"},fa={class:"raw-data"},va={key:1,class:"section"},ga={class:"raw-data"},_a={class:"modal-footer"},ya=U(e({__name:"APICallDetailModal",props:{apiCall:{},trial:{}},emits:["close"],setup(e){const m=e,u=v(()=>{if(!m.apiCall.request)return"No request data";const e=JSON.parse(JSON.stringify(m.apiCall.request));return e.headers&&Object.keys(e.headers).forEach(n=>{const a=n.toLowerCase();(a.includes("authorization")||a.includes("api-key")||a.includes("x-api-key")||a.includes("bearer"))&&(e.headers[n]="[REDACTED]")}),JSON.stringify(e,null,2)});function f(){return m.trial&&m.trial.configurationSnapshots[m.apiCall.configurationIndex]&&m.trial.configurationSnapshots[m.apiCall.configurationIndex].name||`Configuration ${m.apiCall.configurationIndex+1}`}function g(e){const n="string"==typeof e?new Date(e):e;return isNaN(n.getTime())?"Invalid date":n.toLocaleString()}async function _(){const e={id:m.apiCall.id,status:m.apiCall.status,configuration:f(),variables:m.apiCall.variables,variableAttributes:m.apiCall.variableAttributes,prompt:m.apiCall.prompt,request:JSON.parse(u.value),response:m.apiCall.response,result:m.apiCall.result,created:m.apiCall.created,completed:m.apiCall.completed},n=JSON.stringify(e,null,2);try{if(navigator.clipboard&&navigator.clipboard.writeText)return await navigator.clipboard.writeText(n),void z.success("Details copied to clipboard!");const e=document.createElement("textarea");e.value=n,e.style.position="fixed",e.style.left="-999999px",e.style.top="-999999px",document.body.appendChild(e),e.focus(),e.select();const a=document.execCommand("copy");if(document.body.removeChild(e),!a)throw new Error("execCommand failed");z.success("Details copied to clipboard!")}catch(a){console.error("Failed to copy to clipboard:",a),prompt("Copy this text manually:",n)}}return(e,m)=>{const v=p("a-button");return a(),n("div",{class:"modal-overlay",onClick:m[2]||(m[2]=b(n=>e.$emit("close"),["self"]))},[t("div",jn,[t("div",$n,[m[3]||(m[3]=t("h2",null,"API Call Details",-1)),t("button",{class:"close-btn",onClick:m[0]||(m[0]=n=>e.$emit("close"))},"×")]),t("div",Fn,[t("div",Un,[m[11]||(m[11]=t("h3",null,"Overview",-1)),t("div",Dn,[t("div",qn,[m[4]||(m[4]=t("label",null,"Call ID:",-1)),t("span",Ln,i(e.apiCall.id),1)]),t("div",zn,[m[5]||(m[5]=t("label",null,"Status:",-1)),t("span",{class:k(["status-badge",e.apiCall.status])},i(e.apiCall.status),3)]),t("div",Bn,[m[6]||(m[6]=t("label",null,"Configuration:",-1)),t("span",null,i(f()),1)]),t("div",Kn,[m[7]||(m[7]=t("label",null,"Created:",-1)),t("span",null,i(g(e.apiCall.created)),1)]),e.apiCall.completed?(a(),n("div",Yn,[m[8]||(m[8]=t("label",null,"Completed:",-1)),t("span",null,i(g(e.apiCall.completed)),1)])):s("",!0),e.apiCall.completed?(a(),n("div",Gn,[m[9]||(m[9]=t("label",null,"Duration:",-1)),t("span",null,i((y=e.apiCall.completed.getTime()-e.apiCall.created.getTime(),y<1e3?`${y}ms`:`${(y/1e3).toFixed(1)}s`)),1)])):s("",!0),e.apiCall.response?.latencyMs?(a(),n("div",Jn,[m[10]||(m[10]=t("label",null,"API Latency:",-1)),t("span",null,i(e.apiCall.response.latencyMs)+"ms",1)])):s("",!0)])]),t("div",Hn,[m[13]||(m[13]=t("h3",null,"Variables",-1)),t("div",Vn,[(a(!0),n(r,null,l(Object.entries(e.apiCall.variables),([e,s])=>(a(),n("div",{key:e,class:"variable-item"},[t("label",null,i(e)+":",1),t("span",Wn,i(s),1)]))),128))]),e.apiCall.variableAttributes&&Object.keys(e.apiCall.variableAttributes).length>0?(a(),n("div",Xn,[m[12]||(m[12]=t("h4",null,"Variable Attributes",-1)),(a(!0),n(r,null,l(Object.entries(e.apiCall.variableAttributes),([e,s])=>(a(),n("div",{key:e,class:"attribute-group"},[t("h5",null,i(e),1),t("div",Qn,[(a(!0),n(r,null,l(Object.entries(s),([e,s])=>(a(),n("div",{key:e,class:"attribute-item"},[t("label",null,i(e)+":",1),t("span",null,i(s),1)]))),128))])]))),128))])):s("",!0)]),t("div",Zn,[m[14]||(m[14]=t("h3",null,"Resolved Prompt",-1)),t("div",ea,i(e.apiCall.prompt),1)]),e.apiCall.response||e.apiCall.result?(a(),n("div",na,[m[20]||(m[20]=t("h3",null,"Response",-1)),e.apiCall.response?(a(),n("div",aa,[t("div",ta,[t("div",sa,[m[15]||(m[15]=t("label",null,"HTTP Status:",-1)),t("span",null,i(e.apiCall.response.status),1)]),t("div",ra,[m[16]||(m[16]=t("label",null,"Latency:",-1)),t("span",null,i(e.apiCall.response.latencyMs)+"ms",1)])])])):s("",!0),e.apiCall.result?(a(),n("div",la,[!1===e.apiCall.result.success?(a(),n("div",ia,[m[18]||(m[18]=t("h4",null,"Error",-1)),t("div",oa,i(e.apiCall.result.error),1),e.apiCall.response?(a(),n("div",pa,[m[17]||(m[17]=t("h5",null,"Raw Response:",-1)),t("pre",ca,i(JSON.stringify(e.apiCall.response,null,2)),1)])):s("",!0)])):s("",!0),e.apiCall.result.content?(a(),n("div",da,[m[19]||(m[19]=t("h4",null,"Content",-1)),t("div",ma,i(e.apiCall.result.content),1)])):s("",!0)])):s("",!0)])):s("",!0),t("div",ua,[m[21]||(m[21]=t("h3",null,"Raw Request",-1)),t("pre",fa,i(u.value),1)]),e.apiCall.response?(a(),n("div",va,[m[22]||(m[22]=t("h3",null,"Raw Response",-1)),t("pre",ga,i(JSON.stringify(e.apiCall.response,null,2)),1)])):s("",!0)]),t("div",_a,[o(v,{onClick:m[1]||(m[1]=n=>e.$emit("close")),size:"large",class:"footer-button"},{default:c(()=>m[23]||(m[23]=[d(" Close ")])),_:1,__:[23]}),o(v,{type:"primary",onClick:_,size:"large",class:"footer-button footer-button-primary"},{default:c(()=>m[24]||(m[24]=[d(" Copy Details ")])),_:1,__:[24]})])])]);var y}}}),[["__scopeId","data-v-c57e27a2"]]);export{ya as A,rn as T,hn as _,Mn as a,Cn as p};
