import{u as e,b as n,a as t,P as a}from"./designs-db-C9vn5UHT.js";import{u as s}from"./variables-db-B0SlkRNQ.js";import{u as r}from"./models-db-B5PzUypc.js";import{u as l,T as o}from"./trials-db-D4cG7WLs.js";import{p as i}from"./registry-BhCALIg_.js";import{u as p}from"./settings-db-AuQzS6Jq.js";import{g as c}from"./defaultData-DyqJdH2z.js";import{M as d}from"./ModelSelectionTable-CfH6A8xC.js";import{c as m}from"./cost-formatting-D-OPWEvg.js";import{C as u,_ as f,b as v}from"./index-DfsMFxyB.js";import{d as g,f as _,c as y,w as h,o as b,S as k,U as x,X as P,W as C,Y as w,k as S,Z as I,_ as A,a6 as E,G as O,a5 as T,F as N,u as M,B as R,a9 as j}from"./vendor-l9j4AUWK.js";const $={class:"trial-top-bar"},F={class:"top-bar-left"},D={key:0,class:"header-label"},U={key:1,class:"variable-summary"},L={class:"top-bar-right"},q={class:"trial-modal-content"},z={class:"modal-main"},K={class:"left-panel"},B={key:0,class:"guide-state"},Y={key:1,class:"guide-state"},G={key:2,class:"section dense-section"},J={key:0,class:"param-section"},H={key:5,class:"text-secondary text-xs"},V={key:6,class:"param-description text-secondary"},X={class:"param-section"},W={class:"response-mode-option-compact"},Z={class:"mode-label"},Q={class:"mode-desc"},ee={key:1,class:"param-section"},ne={key:5,class:"text-secondary text-xs"},te={key:6,class:"param-description text-secondary"},ae={class:"param-section"},se={class:"api-preview"},re={class:"action-section"},le={class:"right-panel"},oe={class:"section dense-section"},ie={key:0,class:"cost-calculation-compact"},pe={class:"cost-row"},ce={key:0,class:"cost-total"},de={class:"bottom-panel"},me={class:"section dense-section"},ue={key:1,class:"config-list-compact"},fe={class:"config-compact-left"},ve={class:"config-name"},ge={class:"config-model"},_e={key:0,class:"config-param"},ye={key:1,class:"config-param"},he={class:"config-compact-right"},be={class:"config-total-cost"},ke={key:2,class:"trial-summary-compact"},xe={class:"summary-row"},Pe={class:"summary-item"},Ce={class:"summary-item"},we={class:"summary-item"},Se={class:"text-primary"},Ie={class:"summary-item"},Ae={class:"text-success"},Ee={class:"modal-footer"},Oe=f(g({__name:"TrialCreationModal",props:{trialToDuplicate:{},initialDesignId:{}},emits:["close","created"],setup(a,{emit:o}){const f=a,v=o,g=e(),R=s(),j=r(),Oe=l(),Te=p(),Ne=_(""),Me=_(f.initialDesignId||""),Re=_(null),je=_({}),$e=_("text"),Fe=_([]),De=y(()=>g.designs),Ue=y(()=>j.modelsByProvider),Le=y(()=>De.value.find(e=>e.id===Me.value)),qe=y(()=>{if(!Le.value?.variableBindings)return 0;let e=1;for(const[,n]of Object.entries(Le.value.variableBindings))if("list"===n.type&&n.listId){const t=R.lists.find(e=>e.id===n.listId);e*=t?.itemCount||0}else"direct"===n.type&&n.values&&(e*=n.values.length);return e}),ze=y(()=>{const e=[];return Object.entries(Ue.value).forEach(([,n])=>{const t=n.filter(e=>e.enabled);e.push(...t)}),e}),Ke=y(()=>Re.value?i.getBasicParameters(Re.value.provider,Re.value.modelId):{}),Be=y(()=>Re.value?i.getAdvancedParameters(Re.value.provider,Re.value.modelId):{}),Ye=y(()=>Re.value?i.getResponseModes(Re.value.provider):{}),Ge=y(()=>{if(!Re.value)return"";try{const e=i.applyResponseMode(Re.value.provider,$e.value,je.value),t=n.buildAPIRequest({id:"preview",name:"Preview",provider:Re.value.provider,model:Re.value.modelId,params:e,created_at:new Date},"{{prompt}}",Te.getApiKey(Re.value.provider),Te.getBaseUrl(Re.value.provider));return JSON.stringify(t.body,null,2)}catch(e){return"Error generating preview"}}),Je=y(()=>Ne.value.trim().length>0),He=y(()=>""!==Me.value),Ve=y(()=>null!==Re.value),Xe=y(()=>qe.value),We=y(()=>Fe.value.length*Xe.value),Ze=y(()=>{let e=0;return Fe.value.forEach(n=>{const t=ln(n);t&&(e+=t*Xe.value)}),e}),Qe=y(()=>Je.value&&He.value&&Fe.value.length>0&&qe.value>0),en=y(()=>{if(!Re.value)return null;const e=Re.value;if(!e.capabilities?.inputCostPerToken||!e.capabilities?.outputCostPerToken)return null;if(!Le.value?.tokenEstimate?.avgTokens)return null;const n=Le.value.tokenEstimate.avgTokens,a=t.getOutputTokenLimit(e.provider,e.modelId,je.value||{});return m(n,a,e.capabilities.inputCostPerToken,e.capabilities.outputCostPerToken)});function nn(e,n){const t=je.value[e];if(t)try{const a=JSON.parse(t);if("array"===n&&!Array.isArray(a))throw new Error("Value must be an array");if("object"===n&&"object"!=typeof a)throw new Error("Value must be an object");je.value[e]=a}catch(a){console.error(`Invalid JSON for ${e}:`,a)}}function tn(e){Re.value=e,je.value={},$e.value="text";const n=(e,t="")=>{Object.entries(e).forEach(([e,a])=>{const s=t?`${t}.${e}`:e;"object"===a.type&&a.properties?n(a.properties,s):void 0!==a.default&&(je.value[s]=a.default)})},t=i.getParametersForModel(e.provider,e.modelId);n(t)}function an(){}function sn(){if(!Re.value)return;const e=i.applyResponseMode(Re.value.provider,$e.value,je.value),n=Ye.value[$e.value]?.label||$e.value;Fe.value.push({name:`${Re.value.displayName} (${n})`,provider:Re.value.provider,modelId:Re.value.modelId,parameters:{...e}}),Re.value=null,je.value={},$e.value="text"}function rn(e){Fe.value.splice(e,1)}function ln(e){const n=j.getModel(e.provider,e.modelId);if(!n?.capabilities?.inputCostPerToken||!n?.capabilities?.outputCostPerToken)return null;if(!Le.value?.tokenEstimate?.avgTokens)return null;const a=Le.value.tokenEstimate.avgTokens,s=t.getOutputTokenLimit(n.provider,n.modelId,e.parameters||{});return m(a,s,n.capabilities.inputCostPerToken,n.capabilities.outputCostPerToken)}async function on(){try{const e=await Oe.createTrial({name:Ne.value,designId:Me.value,configurations:JSON.parse(JSON.stringify(Fe.value))});v("created",e),v("close")}catch(e){console.error("Failed to create trial:",e),alert("Failed to create trial: "+(e instanceof Error?e.message:"Unknown error"))}}return h(()=>f.trialToDuplicate,async e=>{if(e){e.designSnapshot,Ne.value=`${e.name} (Copy)`,await g.initialize();const n=e.designSnapshot?.originalId;Me.value=n,Fe.value=(e.configurationSnapshots||e.configurations||[]).map(e=>({...e,id:c()}))}},{immediate:!0}),h(Me,async e=>{if(!e)return;const n=De.value.find(n=>n.id===e);if(!n)return;if(!n.tokenEstimate&&n.variableBindings&&Object.keys(n.variableBindings).length>0)try{const e=await t.calculateDesignTokens(n);await g.updateDesign(n.id,{tokenEstimate:e})}catch(o){console.warn("Failed to calculate token estimate:",o)}const a=Oe.trials,s=n.name.toLowerCase(),r=new RegExp(`^${s}\\s+(\\d+)$`,"i");let l=0;for(const t of a){const e=t.name.match(r);if(e){const n=parseInt(e[1]);n>l&&(l=n)}}Ne.value=`${s} ${l+1}`}),h(()=>f.initialDesignId,e=>{e&&(Me.value=e)},{immediate:!0}),b(async()=>{await g.initialize(),await R.initialize(),await Oe.initialize(),0===j.enabledModels.length&&await j.ensureDefaultsEnabled()}),(e,t)=>{const a=x("a-select-option"),s=x("a-select"),r=x("a-input"),l=x("a-input-number"),o=x("a-switch"),p=x("a-textarea"),c=x("a-form-item"),m=x("a-col"),f=x("a-row"),v=x("a-form"),g=x("a-collapse-panel"),_=x("a-collapse"),y=x("a-button"),h=x("a-empty"),b=x("a-modal");return C(),k(b,{open:!0,title:null,width:"95vw",style:{top:"2.5vh",maxWidth:"none"},bodyStyle:{height:"95vh",padding:"0",overflow:"hidden"},footer:null,maskClosable:!1,onCancel:t[5]||(t[5]=n=>e.$emit("close"))},{default:P(()=>[w("div",$,[w("div",F,[t[6]||(t[6]=w("span",{class:"create-trial-title"},"Create Trial",-1)),t[7]||(t[7]=w("span",{class:"top-bar-separator"},"|",-1)),t[8]||(t[8]=w("span",{class:"header-label"},"Select Design",-1)),S(s,{value:Me.value,"onUpdate:value":t[0]||(t[0]=e=>Me.value=e),placeholder:"Select Design",size:"large",class:"design-select","dropdown-class-name":"design-select-dropdown",showSearch:"","filter-option":(e,n)=>(n?.label||n?.value||"").toLowerCase().includes(e.toLowerCase())},{default:P(()=>[(C(!0),I(N,null,E(De.value,e=>(C(),k(a,{key:e.id,value:e.id,label:e.name},{default:P(()=>[O(T(e.name),1)]),_:2},1032,["value","label"]))),128))]),_:1},8,["value","filter-option"]),t[9]||(t[9]=w("span",{class:"header-label"},"Trial Name",-1)),S(r,{value:Ne.value,"onUpdate:value":t[1]||(t[1]=e=>Ne.value=e),placeholder:"Auto-generated...",size:"large",class:"trial-name-input"},null,8,["value"]),Le.value?(C(),I("span",D,"Design Variables")):A("",!0),Le.value?(C(),I("span",U," ("+T(qe.value)+" conditions × "+T(Le.value?.tokenEstimate?.avgTokens||"?")+" tokens/prompt = "+T(qe.value*(Le.value?.tokenEstimate?.avgTokens||0))+" input tokens) ",1)):A("",!0)]),w("div",L,[S(M(u),{onClick:t[2]||(t[2]=n=>e.$emit("close")),class:"close-icon"})])]),w("div",q,[w("div",z,[w("div",K,[Me.value?Re.value?(C(),I("div",G,[w("h3",null,"Configure: "+T(Re.value.displayName),1),Object.keys(Ke.value).length>0?(C(),I("div",J,[t[12]||(t[12]=w("h4",null,"Basic Parameters",-1)),S(v,{layout:"vertical",size:"large"},{default:P(()=>[S(f,{gutter:[8,8]},{default:P(()=>[(C(!0),I(N,null,E(Ke.value,(e,n)=>(C(),k(m,{key:n,span:12},{default:P(()=>[S(c,{label:n,class:"dense-form-item"},{default:P(()=>["number"===e.type||"integer"===e.type?(C(),k(l,{key:0,value:je.value[n],"onUpdate:value":e=>je.value[n]=e,min:e.min,max:e.max,step:"integer"===e.type?1:.1,placeholder:String(e.default),size:"large",style:{width:"100%"}},null,8,["value","onUpdate:value","min","max","step","placeholder"])):"string"!==e.type||e.enum?"string"===e.type&&e.enum?(C(),k(s,{key:2,value:je.value[n],"onUpdate:value":e=>je.value[n]=e,size:"large"},{default:P(()=>[(C(!0),I(N,null,E(e.enum,e=>(C(),k(a,{key:e,value:e},{default:P(()=>[O(T(e),1)]),_:2},1032,["value"]))),128))]),_:2},1032,["value","onUpdate:value"])):"boolean"===e.type?(C(),k(o,{key:3,checked:je.value[n],"onUpdate:checked":e=>je.value[n]=e,size:"large"},null,8,["checked","onUpdate:checked"])):"array"===e.type||"object"===e.type?(C(),k(p,{key:4,value:je.value[n],"onUpdate:value":e=>je.value[n]=e,placeholder:"array"===e.type?"[...]":"{...}","auto-size":{minRows:1,maxRows:3},size:"large",onBlur:t=>nn(n,e.type)},null,8,["value","onUpdate:value","placeholder","onBlur"])):(C(),I("span",H,T(e.type)+" not supported ",1)):(C(),k(r,{key:1,value:je.value[n],"onUpdate:value":e=>je.value[n]=e,placeholder:String(e.default),size:"large"},null,8,["value","onUpdate:value","placeholder"])),e.description?(C(),I("small",V,T(e.description),1)):A("",!0)]),_:2},1032,["label"])]),_:2},1024))),128))]),_:1})]),_:1})])):A("",!0),w("div",X,[t[13]||(t[13]=w("h4",null,"Response Mode",-1)),S(s,{value:$e.value,"onUpdate:value":t[3]||(t[3]=e=>$e.value=e),onChange:an,size:"large",style:{width:"100%"}},{default:P(()=>[(C(!0),I(N,null,E(Ye.value,(e,n)=>(C(),k(a,{key:n,value:n},{default:P(()=>[w("div",W,[w("span",Z,T(e.label),1),w("span",Q,T(e.description),1)])]),_:2},1032,["value"]))),128))]),_:1},8,["value"])]),Object.keys(Be.value).length>0?(C(),I("div",ee,[S(_,{size:"small",ghost:""},{default:P(()=>[S(g,{key:"advanced",header:"Advanced Parameters"},{default:P(()=>[S(v,{layout:"vertical",size:"large"},{default:P(()=>[S(f,{gutter:[8,8]},{default:P(()=>[(C(!0),I(N,null,E(Be.value,(e,n)=>(C(),k(m,{key:n,span:12},{default:P(()=>[S(c,{label:n,class:"dense-form-item"},{default:P(()=>["number"===e.type||"integer"===e.type?(C(),k(l,{key:0,value:je.value[n],"onUpdate:value":e=>je.value[n]=e,min:e.min,max:e.max,step:"integer"===e.type?1:.1,placeholder:String(e.default),size:"large",style:{width:"100%"}},null,8,["value","onUpdate:value","min","max","step","placeholder"])):"string"!==e.type||e.enum?"string"===e.type&&e.enum?(C(),k(s,{key:2,value:je.value[n],"onUpdate:value":e=>je.value[n]=e,size:"large"},{default:P(()=>[(C(!0),I(N,null,E(e.enum,e=>(C(),k(a,{key:e,value:e},{default:P(()=>[O(T(e),1)]),_:2},1032,["value"]))),128))]),_:2},1032,["value","onUpdate:value"])):"boolean"===e.type?(C(),k(o,{key:3,checked:je.value[n],"onUpdate:checked":e=>je.value[n]=e,size:"large"},null,8,["checked","onUpdate:checked"])):"array"===e.type||"object"===e.type?(C(),k(p,{key:4,value:je.value[n],"onUpdate:value":e=>je.value[n]=e,placeholder:"array"===e.type?"[...]":"{...}","auto-size":{minRows:1,maxRows:3},size:"large",onBlur:t=>nn(n,e.type)},null,8,["value","onUpdate:value","placeholder","onBlur"])):(C(),I("span",ne,T(e.type)+" not supported ",1)):(C(),k(r,{key:1,value:je.value[n],"onUpdate:value":e=>je.value[n]=e,placeholder:String(e.default),size:"large"},null,8,["value","onUpdate:value","placeholder"])),e.description?(C(),I("small",te,T(e.description),1)):A("",!0)]),_:2},1032,["label"])]),_:2},1024))),128))]),_:1})]),_:1})]),_:1})]),_:1})])):A("",!0),w("div",ae,[S(_,{size:"small",ghost:""},{default:P(()=>[S(g,{key:"preview",header:"Raw API Preview"},{default:P(()=>[w("pre",se,T(Ge.value),1)]),_:1})]),_:1})]),w("div",re,[S(y,{type:"primary",onClick:sn,disabled:!Ve.value,size:"large",block:""},{default:P(()=>t[14]||(t[14]=[O(" Add Configuration ")])),_:1,__:[14]},8,["disabled"])])])):(C(),I("div",Y,t[11]||(t[11]=[w("div",{class:"guide-box"},[w("h2",null,"→ CHOOSE A MODEL →"),w("p",null,"Select any model from the right panel")],-1)]))):(C(),I("div",B,t[10]||(t[10]=[w("div",{class:"guide-box"},[w("h2",null,"↑ SELECT A DESIGN ↑"),w("p",null,"Choose from the dropdown above")],-1)])))]),w("div",le,[w("div",oe,[t[17]||(t[17]=w("h3",null,"Select Model",-1)),Re.value&&en.value?(C(),I("div",ie,[w("div",pe,[t[16]||(t[16]=w("span",{class:"cost-label"},"Cost per call:",-1)),w("strong",null,"$"+T(en.value.toFixed(5)),1),Xe.value>0?(C(),I("span",ce,[t[15]||(t[15]=O(" • Total: ")),w("strong",null,"$"+T((en.value*Xe.value).toFixed(3)),1),O(" ("+T(Xe.value)+" calls) ",1)])):A("",!0)])])):A("",!0),S(d,{models:ze.value,"selected-model":Re.value,"is-loading":!1,"total-calls":qe.value,design:Le.value,"model-params":je.value,onModelSelected:tn,onModelConfigured:tn},null,8,["models","selected-model","total-calls","design","model-params"])])])]),w("div",de,[w("div",me,[t[28]||(t[28]=w("h3",null,"Trial Configurations",-1)),0===Fe.value.length?(C(),k(h,{key:0,description:"No configurations added yet. Select a model and configure parameters.",style:{margin:"var(--spacing-md) 0"}},{image:P(()=>t[18]||(t[18]=[w("div",{style:{color:"var(--color-text-disabled)","font-size":"var(--font-size-xxl)"}},"⚙️",-1)])),_:1})):(C(),I("div",ue,[(C(!0),I(N,null,E(Fe.value,(e,a)=>(C(),I("div",{key:a,class:"config-item-compact"},[w("div",fe,[w("strong",ve,T(e.name),1),t[19]||(t[19]=w("span",{class:"config-separator"},"•",-1)),w("span",ge,T(e.provider)+":"+T(e.modelId),1),t[20]||(t[20]=w("span",{class:"config-separator"},"•",-1)),void 0!==e.parameters.temperature?(C(),I("span",_e," T="+T(e.parameters.temperature),1)):A("",!0),void 0!==e.parameters.max_tokens?(C(),I("span",ye," Max="+T(e.parameters.max_tokens),1)):A("",!0)]),w("div",he,[w("span",be,[t[21]||(t[21]=O(" Total: ")),w("strong",null,"$"+T(((ln(e)||0)*Xe.value).toFixed(2)),1)]),S(y,{type:"text",size:"small",onClick:e=>function(e){const t=Fe.value[e],a=j.getModel(t.provider,t.modelId);if(a){Re.value=a,je.value=n.flattenParameters(t.parameters);const e=i.getResponseModes(t.provider);$e.value="text";for(const[n,a]of Object.entries(e)){const e=a.parameters||{};if(Object.keys(e).every(e=>e in t.parameters)&&Object.keys(e).length>0){$e.value=n;break}}}rn(e)}(a)},{default:P(()=>t[22]||(t[22]=[O("Edit")])),_:2,__:[22]},1032,["onClick"]),S(y,{type:"text",size:"small",danger:"",onClick:e=>rn(a)},{default:P(()=>t[23]||(t[23]=[O("Remove")])),_:2,__:[23]},1032,["onClick"])])]))),128))])),Fe.value.length>0&&qe.value>0?(C(),I("div",ke,[w("div",xe,[w("div",Pe,[t[24]||(t[24]=w("span",{class:"summary-label"},"Variables:",-1)),w("strong",null,T(Xe.value),1)]),w("div",Ce,[t[25]||(t[25]=w("span",{class:"summary-label"},"Configs:",-1)),w("strong",null,T(Fe.value.length),1)]),w("div",we,[t[26]||(t[26]=w("span",{class:"summary-label"},"Total Calls:",-1)),w("strong",Se,T(We.value),1)]),w("div",Ie,[t[27]||(t[27]=w("span",{class:"summary-label"},"Cost:",-1)),w("strong",Ae,"$"+T(Ze.value.toFixed(2)),1)])])])):A("",!0)])]),w("div",Ee,[S(y,{onClick:t[4]||(t[4]=n=>e.$emit("close")),size:"large",class:"footer-button"},{default:P(()=>t[29]||(t[29]=[O(" Cancel ")])),_:1,__:[29]}),S(y,{type:"primary",onClick:on,disabled:!Qe.value,size:"large",class:"footer-button footer-button-primary"},{default:P(()=>t[30]||(t[30]=[O(" Create Trial ")])),_:1,__:[30]},8,["disabled"])])])]),_:1})}}}),[["__scopeId","data-v-4c260a1f"]]),Te={class:"trial-overview"},Ne={class:"overview-section"},Me={class:"cost-value"},Re={class:"overview-section"},je={class:"progress-details"},$e={class:"progress-stats"},Fe={class:"configurations-section"},De={key:0,class:"model-info"},Ue={key:0},Le={class:"prompt-section"},qe={key:0,class:"variables-section"},ze=g({__name:"TrialDetailModal",props:{trial:{}},emits:["close","updated"],setup(e){const n=e,t=[{title:"Model",key:"model",width:200},{title:"Parameters",key:"params",width:300},{title:"Cost/Call",key:"cost",width:100,align:"right"}],a=y(()=>0===n.trial.progress.total?0:Math.round(n.trial.progress.completed/n.trial.progress.total*100));function s(e){const n=Object.entries(e).map(([e,n])=>`${e}: ${n}`).join(", ");return n.length>50?n.substring(0,50)+"...":n}function r(){const e=n.trial.variableSnapshots;return e&&0!==e.length?e.map(e=>({variable:e.variableName,listName:e.originalListName,count:e.data.itemCount})):[]}return(e,n)=>{const l=x("a-button"),o=x("a-tag"),i=x("a-descriptions-item"),p=x("a-descriptions"),c=x("a-progress"),d=x("a-typography-text"),m=x("a-table"),u=x("a-typography-paragraph"),f=x("a-list-item-meta"),v=x("a-list-item"),g=x("a-list"),_=x("a-modal");return C(),k(_,{open:!0,title:e.trial.name,width:"95vw",centered:!0,onCancel:n[1]||(n[1]=n=>e.$emit("close")),"wrap-class-name":"trial-detail-modal","body-style":{height:"90vh",overflow:"auto"}},{footer:P(()=>[S(l,{onClick:n[0]||(n[0]=n=>e.$emit("close")),size:"large"},{default:P(()=>n[2]||(n[2]=[O(" Close ")])),_:1,__:[2]})]),default:P(()=>[w("div",Te,[w("div",Ne,[n[3]||(n[3]=w("h3",null,"Trial Information",-1)),S(p,{column:2,size:"small",bordered:""},{default:P(()=>[S(i,{label:"Status"},{default:P(()=>{return[S(o,{color:(n=e.trial.status,{completed:"success",failed:"error",running:"processing",cancelled:"default",draft:"default",pending:"processing",paused:"warning"}[n]||"default")},{default:P(()=>[O(T(e.trial.status.toUpperCase()),1)]),_:1},8,["color"])];var n}),_:1}),S(i,{label:"Design"},{default:P(()=>[O(T(e.trial.designSnapshot.originalName),1)]),_:1}),S(i,{label:"Created"},{default:P(()=>{return[O(T((n=e.trial.created,new Date(n).toLocaleString())),1)];var n}),_:1}),S(i,{label:"Estimated Cost"},{default:P(()=>[w("span",Me,"$"+T(e.trial.estimatedCost.toFixed(3)),1)]),_:1})]),_:1})]),w("div",Re,[n[4]||(n[4]=w("h3",null,"Progress",-1)),w("div",je,[S(c,{percent:a.value,status:"failed"===e.trial.status?"exception":"active",size:"small"},null,8,["percent","status"]),w("div",$e,[S(o,null,{default:P(()=>[O(T(e.trial.progress.completed)+" / "+T(e.trial.progress.total)+" completed",1)]),_:1}),e.trial.progress.networkErrors>0?(C(),k(o,{key:0,color:"error"},{default:P(()=>[O(T(e.trial.progress.networkErrors)+" network errors ",1)]),_:1})):A("",!0)])])])]),w("div",Fe,[w("h3",null,"Configurations ("+T(e.trial.configurationSnapshots.length)+")",1),S(m,{columns:t,"data-source":e.trial.configurationSnapshots,pagination:!1,size:"small",scroll:{y:300},"row-key":"id"},{bodyCell:P(({column:e,record:n})=>["model"===e.key?(C(),I("div",De,[w("strong",null,T(n.provider),1),w("small",null,T(n.modelId),1)])):A("",!0),"params"===e.key?(C(),k(d,{key:1,code:"",class:"params-preview"},{default:P(()=>[O(T(s(n.parameters)),1)]),_:2},1024)):A("",!0),"cost"===e.key?(C(),I(N,{key:2},[(C(),I("span",Ue," $"+T(.001.toFixed(4)),1))],64)):A("",!0)]),_:1},8,["data-source"])]),w("div",Le,[n[5]||(n[5]=w("h3",null,"Prompt Template",-1)),S(u,{code:"",class:"prompt-template"},{default:P(()=>[O(T(e.trial.designSnapshot.promptTemplate),1)]),_:1})]),e.trial.variableSnapshots?.length?(C(),I("div",qe,[n[6]||(n[6]=w("h3",null,"Variables",-1)),S(g,{"data-source":r(),size:"small",split:!1},{renderItem:P(({item:e})=>[S(v,null,{default:P(()=>[S(f,null,{title:P(()=>[S(o,{color:"blue"},{default:P(()=>[O(T(e.variable),1)]),_:2},1024)]),description:P(()=>[w("span",null,T(e.listName)+" ("+T(e.count)+" values)",1)]),_:2},1024)]),_:2},1024)]),_:1},8,["data-source"])])):A("",!0)]),_:1},8,["title"])}}});class Ke{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=new o({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e),t=this.extractUniqueVariables(n),a=e.configurationSnapshots.map(e=>({provider:e.provider,modelId:e.modelId,displayName:e.name,parameters:e.parameters})),s=new Set(a.map(e=>e.provider)),r={};for(const l of s){const e=i.getProvider(l);e&&(r[l]=this.buildProviderConfig(l,e))}return{experiment:{promptTemplate:e.designSnapshot.promptTemplate,variables:t},models:a,providerConfigs:r}}static extractUniqueVariables(e){const n={};for(const a of e)for(const[e,t]of Object.entries(a.variables))n[e]||(n[e]=new Set),n[e].add(t);const t={};for(const[a,s]of Object.entries(n))t[a]=Array.from(s).sort();return t}static buildProviderConfig(e,n){const t=n.requestTransform||{},a=n.auth||{type:"none"};let s="direct";"messages"===t.promptKey&&t.wrapPrompt?s="messages":"input"===t.promptKey&&(s="input");let r,l,o="root";"ollama-chat"===e?(o="options",r={max_tokens:"num_predict",max_completion_tokens:"num_predict"}):"ollama-generate"===e&&(o="mixed",l={root:["model","prompt","stream","format","raw"],options:["temperature","num_predict","top_k","top_p"]},r={max_tokens:"num_predict",max_completion_tokens:"num_predict"});const i=Object.values(n.responseModes||{})[0],p=this.parseResponsePath(i?.responseTransform?.contentPath),c=i?.responseTransform?.fallbackPaths?.map(e=>this.parseResponsePath(e)),d=n.api.baseUrl+(n.api.endpoints.chat||n.api.endpoints.generate||"");return{name:n.name,endpoint:d,auth:{type:a.type,header:a.header,prefix:"bearer"===a.type?"Bearer":void 0},headers:n.headers,request:{modelPrefixStrip:!0,promptFormat:s,messageRole:t.messageRole,paramLocation:o,paramRenames:r,mixedParams:l},response:{successPath:p,fallbackPaths:c,errorPath:["error","message"]}}}static parseResponsePath(e){return e?e.split(/[\.\[\]]/).filter(Boolean).map(e=>{const n=parseInt(e);return isNaN(n)?e:n}):["content"]}static generateScript(e,n){const t=(new Date).toISOString(),a=JSON.stringify(e.experiment.variables,null,4),s=JSON.stringify(e.models,null,4),r=JSON.stringify(e.providerConfigs,null,4);return`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Simple Mode\n=====================================\nGenerated by Auditomatic Lite on ${t}\n\nThis script reproduces your experiment by generating API calls from variables.\nPerfect for understanding, modifying, and extending your experiments.\n\nOriginal trial: ${n}\n"""\n\nimport os\nimport json\nimport time\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\nAPI_KEYS = {\n${Object.keys(e.providerConfigs).map(e=>{const n=e.split("-")[0].toUpperCase();return`    "${e}": os.environ.get("${n}_API_KEY", ""),`}).join("\n")}\n}\n\n# Your experiment design\nEXPERIMENT = {\n    "prompt_template": "${e.experiment.promptTemplate.replace(/"/g,'\\"')}",\n    "variables": ${a}\n}\n\n# Models to test\nMODELS = ${s}\n\n# Provider configurations (how to talk to each API)\nPROVIDER_CONFIGS = ${r}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\ndef make_api_call(provider_id: str, model: str, prompt: str, params: dict) -> dict:\n    """\n    Universal API caller that handles all provider quirks.\n    \n    Returns dict with 'success', 'content', 'error', and timing info.\n    """\n    config = PROVIDER_CONFIGS[provider_id]\n    \n    # Build headers\n    headers = {"Content-Type": "application/json"}\n    \n    # Add authentication\n    auth = config["auth"]\n    if auth["type"] == "bearer":\n        api_key = API_KEYS.get(provider_id, "")\n        if not api_key:\n            return {"success": False, "error": f"No API key for {provider_id}"}\n        headers[auth["header"]] = f"{auth['prefix']} {api_key}"\n    elif auth["type"] == "header":\n        api_key = API_KEYS.get(provider_id, "")\n        if not api_key:\n            return {"success": False, "error": f"No API key for {provider_id}"}\n        headers[auth["header"]] = api_key\n    \n    # Add provider-specific headers\n    if config.get("headers"):\n        headers.update(config["headers"])\n    \n    # Build request body\n    request = config["request"]\n    \n    # Strip provider prefix from model\n    if request.get("modelPrefixStrip"):\n        model = model.split(":", 1)[-1]\n    \n    body = {"model": model}\n    \n    # Format prompt\n    if request["promptFormat"] == "messages":\n        body["messages"] = [{"role": request.get("messageRole", "user"), "content": prompt}]\n    elif request["promptFormat"] == "direct":\n        body["prompt"] = prompt\n    elif request["promptFormat"] == "input":\n        body["input"] = prompt\n    \n    # Handle parameters\n    processed_params = params.copy()\n    \n    # Apply renames\n    if request.get("paramRenames"):\n        for old_key, new_key in request["paramRenames"].items():\n            if old_key in processed_params:\n                processed_params[new_key] = processed_params.pop(old_key)\n    \n    # Place parameters\n    if request["paramLocation"] == "root":\n        body.update(processed_params)\n    elif request["paramLocation"] == "options":\n        body["options"] = processed_params\n    elif request.get("mixedParams"):\n        mixed = request["mixedParams"]\n        for key, value in processed_params.items():\n            if key in mixed.get("root", []):\n                body[key] = value\n            else:\n                if "options" not in body:\n                    body["options"] = {}\n                body["options"][key] = value\n    \n    # Make request\n    start_time = time.time()\n    try:\n        response = requests.post(\n            config["endpoint"],\n            headers=headers,\n            json=body,\n            timeout=30\n        )\n        latency_ms = (time.time() - start_time) * 1000\n        \n        if response.ok:\n            data = response.json()\n            content = extract_from_path(data, config["response"]["successPath"])\n            \n            # Try fallback paths\n            if content is None and config["response"].get("fallbackPaths"):\n                for path in config["response"]["fallbackPaths"]:\n                    content = extract_from_path(data, path)\n                    if content is not None:\n                        break\n            \n            return {\n                "success": True,\n                "content": content or "",\n                "latency_ms": latency_ms,\n                "status_code": response.status_code\n            }\n        else:\n            return {\n                "success": False,\n                "error": f"HTTP {response.status_code}: {response.text[:200]}",\n                "latency_ms": latency_ms,\n                "status_code": response.status_code\n            }\n            \n    except Exception as e:\n        return {\n            "success": False,\n            "error": str(e),\n            "latency_ms": (time.time() - start_time) * 1000\n        }\n\ndef extract_from_path(data: Any, path: List[Any]) -> Optional[str]:\n    """Extract value from nested data using a path like ['choices', 0, 'message', 'content']"""\n    try:\n        current = data\n        for key in path:\n            if isinstance(current, dict):\n                current = current[key]\n            elif isinstance(current, list):\n                current = current[int(key)]\n            else:\n                return None\n        return str(current) if current is not None else None\n    except (KeyError, IndexError, TypeError):\n        return None\n\ndef generate_prompts():\n    """Generate all prompts from template and variables"""\n    template = EXPERIMENT["prompt_template"]\n    variables = EXPERIMENT["variables"]\n    \n    # Get variable names from template\n    import re\n    var_names = re.findall(r'{{(\\w+)}}', template)\n    \n    # Generate all combinations\n    from itertools import product\n    \n    var_lists = [variables[var] for var in var_names]\n    for values in product(*var_lists):\n        var_dict = dict(zip(var_names, values))\n        \n        # Replace variables in template\n        prompt = template\n        for var, val in var_dict.items():\n            prompt = prompt.replace(f"{{{{{var}}}}}", str(val))\n        \n        yield prompt, var_dict\n\ndef run_experiment():\n    """Run the full experiment"""\n    results = []\n    total_calls = len(MODELS) * len(list(generate_prompts()))\n    current = 0\n    \n    print(f"Running experiment with {len(MODELS)} models and {total_calls} total API calls")\n    print("=" * 60)\n    \n    for model_config in MODELS:\n        print(f"\\nTesting {model_config['displayName']}...")\n        \n        for prompt, variables in generate_prompts():\n            current += 1\n            print(f"[{current}/{total_calls}] {prompt[:50]}...", end=" ")\n            \n            # Make API call\n            result = make_api_call(\n                model_config["provider"],\n                model_config["modelId"],\n                prompt,\n                model_config["parameters"]\n            )\n            \n            # Collect results\n            results.append({\n                "timestamp": datetime.now(),\n                "provider": model_config["provider"],\n                "model": model_config["modelId"],\n                "model_name": model_config["displayName"],\n                "prompt": prompt,\n                "response": result.get("content", ""),\n                "success": result.get("success", False),\n                "error": result.get("error", ""),\n                "latency_ms": result.get("latency_ms", 0),\n                "status_code": result.get("status_code", 0),\n                **variables  # Add variables as columns\n            })\n            \n            # Show result\n            if result["success"]:\n                print(f"✓ {result['content'][:30]}")\n            else:\n                print(f"✗ {result['error'][:30]}")\n            \n            # Rate limiting\n            time.sleep(0.1)\n    \n    return results\n\ndef save_results(results: List[Dict[str, Any]], format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_results_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for API keys\n    missing_keys = []\n    for model in MODELS:\n        provider = model["provider"]\n        if provider not in API_KEYS or not API_KEYS[provider]:\n            missing_keys.append(provider)\n    \n    if missing_keys:\n        print("WARNING: Missing API keys for:", ", ".join(set(missing_keys)))\n        print("Set them in the API_KEYS dict or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Run experiment\n    results = run_experiment()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if 'latency_ms' in df.columns:\n            print(f"Avg latency: {df['latency_ms'].mean():.1f}ms")\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class Be{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=[],t=new a,s=new o({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e);let r=0;for(const a of e.configurationSnapshots){const o=i.getProvider(a.provider);if(o)for(const i of s){r++;let s=e.designSnapshot.promptTemplate;for(const[e,n]of Object.entries(i.variables))s=s.replace(new RegExp(`{{${e}}}`,"g"),n);try{const e={id:"export-config",name:a.name,provider:a.provider,model:a.modelId,params:a.parameters,created_at:new Date},l=t.buildAPIRequest(e,s),p={};for(const[n,t]of Object.entries(l.headers))"Authorization"===n&&t.startsWith("Bearer ")?p[n]=`Bearer $${a.provider.split("-")[0].toUpperCase()}_API_KEY`:n===o.auth.header&&"header"===o.auth.type?p[n]=`$${a.provider.split("-")[0].toUpperCase()}_API_KEY`:p[n]=t;const c=this.parseResponsePath(this.getDefaultResponsePath(a.provider));n.push({id:`call_${String(r).padStart(3,"0")}`,provider:a.provider,endpoint:l.url,headers:p,body:l.body,responsePath:c,metadata:{variables:i.variables,modelName:a.modelId,configName:a.name}})}catch(l){console.warn(`Failed to build API call for ${a.name}:`,l)}}}return{apiCalls:n}}static parseResponsePath(e){return e.split(/[\.\[\]]/).filter(Boolean).map(e=>{const n=parseInt(e);return isNaN(n)?e:n})}static getDefaultResponsePath(e){switch(e){case"openai-chat":case"openrouter":return"choices[0].message.content";case"openai-responses":return"output[0].content[0].text";case"anthropic":return"content[0].text";case"ollama-chat":return"message.content";case"ollama-generate":return"response";default:return"content"}}static generateScript(e,n){const t=(new Date).toISOString(),a=JSON.stringify(e.apiCalls,null,4),s=[...new Set(e.apiCalls.map(e=>e.provider))];return`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Literal Mode\n======================================\nGenerated by Auditomatic Lite on ${t}\n\nThis script contains the EXACT API calls from your experiment.\nPerfect for bit-for-bit reproduction, debugging, and comparing results.\n\nOriginal trial: ${n}\nTotal API calls: ${e.apiCalls.length}\n"""\n\nimport os\nimport json\nimport time\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\nAPI_KEYS = {\n${s.map(e=>{const n=e.split("-")[0].toUpperCase();return`    "${n}": os.environ.get("${n}_API_KEY", ""),`}).join("\n")}\n}\n\n# Pre-computed API calls from your experiment\nAPI_CALLS = ${a}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\ndef execute_literal_calls():\n    """Execute pre-serialized API calls exactly as specified"""\n    results = []\n    total = len(API_CALLS)\n    \n    print(f"Executing {total} pre-computed API calls...")\n    print("=" * 60)\n    \n    for i, call in enumerate(API_CALLS):\n        print(f"[{i+1}/{total}] {call['metadata']['configName']} - ", end="")\n        \n        # Replace API key placeholders in headers\n        headers = {}\n        for key, value in call["headers"].items():\n            if "\\$" in str(value):\n                # Extract provider name from placeholder\n                for provider_key, api_key in API_KEYS.items():\n                    placeholder = f"\\\${provider_key}_API_KEY"\n                    if placeholder in value:\n                        headers[key] = value.replace(placeholder, api_key)\n                        break\n                else:\n                    headers[key] = value\n            else:\n                headers[key] = value\n        \n        # Check if we have required API key\n        provider_base = call["provider"].split("-")[0].upper()\n        if provider_base in ["OPENAI", "ANTHROPIC", "OPENROUTER"] and not API_KEYS.get(provider_base):\n            results.append({\n                "call_id": call["id"],\n                "timestamp": datetime.now(),\n                "provider": call["provider"],\n                "model": call["metadata"]["modelName"],\n                "config_name": call["metadata"]["configName"],\n                "prompt": extract_prompt_from_body(call["body"]),\n                "response": "",\n                "success": False,\n                "error": f"No API key for {provider_base}",\n                "latency_ms": 0,\n                "status_code": 0,\n                **call["metadata"]["variables"]\n            })\n            print(f"✗ No API key")\n            continue\n        \n        # Make the exact API call\n        start_time = time.time()\n        try:\n            response = requests.post(\n                call["endpoint"],\n                headers=headers,\n                json=call["body"],\n                timeout=30\n            )\n            latency_ms = (time.time() - start_time) * 1000\n            \n            if response.ok:\n                data = response.json()\n                content = extract_from_path(data, call["responsePath"])\n                \n                results.append({\n                    "call_id": call["id"],\n                    "timestamp": datetime.now(),\n                    "provider": call["provider"],\n                    "model": call["metadata"]["modelName"],\n                    "config_name": call["metadata"]["configName"],\n                    "prompt": extract_prompt_from_body(call["body"]),\n                    "response": content or "",\n                    "success": True,\n                    "error": "",\n                    "latency_ms": latency_ms,\n                    "status_code": response.status_code,\n                    "full_response": json.dumps(data)[:500],  # First 500 chars\n                    **call["metadata"]["variables"]\n                })\n                print(f"✓ {(content or '')[:30]}")\n            else:\n                results.append({\n                    "call_id": call["id"],\n                    "timestamp": datetime.now(),\n                    "provider": call["provider"],\n                    "model": call["metadata"]["modelName"],\n                    "config_name": call["metadata"]["configName"],\n                    "prompt": extract_prompt_from_body(call["body"]),\n                    "response": "",\n                    "success": False,\n                    "error": f"HTTP {response.status_code}: {response.text[:200]}",\n                    "latency_ms": latency_ms,\n                    "status_code": response.status_code,\n                    **call["metadata"]["variables"]\n                })\n                print(f"✗ HTTP {response.status_code}")\n                \n        except Exception as e:\n            latency_ms = (time.time() - start_time) * 1000\n            results.append({\n                "call_id": call["id"],\n                "timestamp": datetime.now(),\n                "provider": call["provider"],\n                "model": call["metadata"]["modelName"],\n                "config_name": call["metadata"]["configName"],\n                "prompt": extract_prompt_from_body(call["body"]),\n                "response": "",\n                "success": False,\n                "error": str(e)[:200],\n                "latency_ms": latency_ms,\n                "status_code": 0,\n                **call["metadata"]["variables"]\n            })\n            print(f"✗ {str(e)[:30]}")\n        \n        # Rate limiting\n        time.sleep(0.1)\n    \n    return results\n\ndef extract_prompt_from_body(body: dict) -> str:\n    """Extract the prompt from various request body formats"""\n    # Messages format (OpenAI, Anthropic, etc)\n    if "messages" in body and isinstance(body["messages"], list):\n        for msg in body["messages"]:\n            if msg.get("role") == "user":\n                return msg.get("content", "")\n    \n    # Direct prompt format (Ollama generate)\n    if "prompt" in body:\n        return body["prompt"]\n    \n    # Input format (OpenAI responses)\n    if "input" in body:\n        return body["input"]\n    \n    return ""\n\ndef extract_from_path(data: Any, path: List[Any]) -> Optional[str]:\n    """Extract value from nested data using a path like ['choices', 0, 'message', 'content']"""\n    try:\n        current = data\n        for key in path:\n            if isinstance(current, dict):\n                current = current[key]\n            elif isinstance(current, list):\n                current = current[int(key)]\n            else:\n                return None\n        return str(current) if current is not None else None\n    except (KeyError, IndexError, TypeError):\n        return None\n\ndef save_results(results: List[Dict[str, Any]], format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    # Drop full_response column for cleaner output (except JSON)\n    if format != "json" and "full_response" in df.columns:\n        df = df.drop(columns=["full_response"])\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_literal_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for API keys\n    required_providers = set(call["provider"].split("-")[0].upper() for call in API_CALLS)\n    missing_keys = []\n    for provider in required_providers:\n        if provider not in ["OLLAMA"] and not API_KEYS.get(provider):\n            missing_keys.append(provider)\n    \n    if missing_keys:\n        print("WARNING: Missing API keys for:", ", ".join(missing_keys))\n        print("Set them in the API_KEYS dict or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Execute all calls\n    results = execute_literal_calls()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if df['success'].any():\n            print(f"Avg latency (successful): {df[df['success']]['latency_ms'].mean():.1f}ms")\n        \n        # Group by model\n        print(f"\\nBy Model:")\n        model_summary = df.groupby('config_name')['success'].agg(['count', 'sum', 'mean'])\n        model_summary.columns = ['total', 'successful', 'success_rate']\n        print(model_summary)\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class Ye{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=new o({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e),t=this.extractUniqueVariables(n),a=e.configurationSnapshots.map(e=>{let n,t="text";return e.parameters.response_format?(t="json_mode",n={response_format:e.parameters.response_format}):e.parameters.tools&&(t="function_calling",n={tools:e.parameters.tools,tool_choice:e.parameters.tool_choice}),{provider:e.provider,modelId:e.modelId,displayName:e.name,parameters:this.filterCoreParams(e.parameters),responseMode:t,responseModeParams:n}}),s=new Set(a.map(e=>e.provider)),r={"openai-chat":"openai","openai-responses":"openai",anthropic:"anthropic",openrouter:"openai","ollama-chat":"ollama","ollama-generate":"ollama"},l=[...new Set(Array.from(s).map(e=>r[e]).filter(Boolean))],i={"openai-chat":"OPENAI","openai-responses":"OPENAI",anthropic:"ANTHROPIC",openrouter:"OPENROUTER","ollama-chat":"","ollama-generate":""},p=[...new Set(Array.from(s).map(e=>i[e]).filter(Boolean))];return{experiment:{promptTemplate:e.designSnapshot.promptTemplate,variables:t},models:a,providerLibraries:{required:l,apiKeys:p}}}static extractUniqueVariables(e){const n={};for(const a of e)for(const[e,t]of Object.entries(a.variables))n[e]||(n[e]=new Set),n[e].add(t);const t={};for(const[a,s]of Object.entries(n))t[a]=Array.from(s).sort();return t}static filterCoreParams(e){const n={...e};return delete n.response_format,delete n.tools,delete n.tool_choice,n}static generateScript(e,n){const t=(new Date).toISOString(),a=JSON.stringify(e.experiment.variables,null,4),s=JSON.stringify(e.models,null,4),r=["import os","import json","import time","import pandas as pd","from datetime import datetime"];return e.providerLibraries.required.includes("openai")&&r.push("from openai import OpenAI"),e.providerLibraries.required.includes("anthropic")&&r.push("from anthropic import Anthropic"),e.providerLibraries.required.includes("ollama")&&r.push("import ollama"),`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Native Mode\n=====================================\nGenerated by Auditomatic Lite on ${t}\n\nThis script uses native Python libraries for each provider.\nCleanest code, best for production use.\n\nOriginal trial: ${n}\nRequired packages: ${e.providerLibraries.required.join(", ")}\n"""\n\n${r.join("\n")}\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\n${e.providerLibraries.apiKeys.map(e=>`os.environ.setdefault("${e}_API_KEY", "")  # Set your ${e} API key`).join("\n")}\n\n# Your experiment design\nEXPERIMENT = {\n    "prompt_template": "${e.experiment.promptTemplate.replace(/"/g,'\\"')}",\n    "variables": ${a}\n}\n\n# Models to test\nMODELS = ${s}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\n# Initialize clients\nclients = {}\n\ndef get_client(provider):\n    """Get or create client for provider"""\n    if provider not in clients:\n        if provider in ["openai-chat", "openai-responses"]:\n            clients[provider] = OpenAI()\n        elif provider == "anthropic":\n            clients[provider] = Anthropic()\n        elif provider == "openrouter":\n            clients[provider] = OpenAI(\n                api_key=os.environ.get("OPENROUTER_API_KEY"),\n                base_url="https://openrouter.ai/api/v1"\n            )\n        # Ollama doesn't need a client\n    return clients.get(provider)\n\ndef make_api_call(model_config: dict, prompt: str) -> dict:\n    """Make API call using native provider library"""\n    provider = model_config["provider"]\n    model = model_config["modelId"]\n    params = model_config["parameters"].copy()\n    \n    try:\n        start_time = time.time()\n        \n        if provider == "openai-chat" or provider == "openrouter":\n            client = get_client(provider)\n            \n            # Build messages\n            messages = [{"role": "user", "content": prompt}]\n            \n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["response_format"] = {"type": "json_object"}\n            elif model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                **params\n            )\n            \n            # Extract content based on response mode\n            if model_config["responseMode"] == "function_calling" and response.choices[0].message.tool_calls:\n                content = response.choices[0].message.tool_calls[0].function.arguments\n                if isinstance(content, str):\n                    content = json.loads(content)\n            else:\n                content = response.choices[0].message.content\n            \n        elif provider == "openai-responses":\n            client = get_client(provider)\n            \n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["text"] = {"format": {"type": "json_object"}}\n            elif model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.responses.create(\n                model=model,\n                input=prompt,\n                **params\n            )\n            \n            # Extract content\n            output = response.output\n            if isinstance(output, list) and len(output) > 0:\n                if hasattr(output[0], 'content') and isinstance(output[0].content, list):\n                    content = output[0].content[0].text if hasattr(output[0].content[0], 'text') else str(output[0].content[0])\n                else:\n                    content = str(output[0])\n            else:\n                content = str(output)\n            \n        elif provider == "anthropic":\n            client = get_client(provider)\n            \n            # Build messages\n            messages = [{"role": "user", "content": prompt}]\n            \n            # Handle response modes\n            if model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.messages.create(\n                model=model,\n                messages=messages,\n                **params\n            )\n            \n            # Extract content\n            if model_config["responseMode"] == "function_calling" and hasattr(response.content[0], 'input'):\n                content = response.content[0].input\n            else:\n                content = response.content[0].text\n            \n        elif provider == "ollama-chat":\n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["format"] = "json"\n            \n            # Make call\n            response = ollama.chat(\n                model=model,\n                messages=[{"role": "user", "content": prompt}],\n                **params\n            )\n            \n            # Extract content\n            content = response["message"]["content"]\n            \n        elif provider == "ollama-generate":\n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["format"] = "json"\n            \n            # Make call\n            response = ollama.generate(\n                model=model,\n                prompt=prompt,\n                **params\n            )\n            \n            # Extract content\n            content = response["response"]\n        \n        else:\n            raise ValueError(f"Unknown provider: {provider}")\n        \n        latency_ms = (time.time() - start_time) * 1000\n        \n        return {\n            "success": True,\n            "content": content,\n            "latency_ms": latency_ms\n        }\n        \n    except Exception as e:\n        latency_ms = (time.time() - start_time) * 1000\n        return {\n            "success": False,\n            "content": "",\n            "error": str(e),\n            "latency_ms": latency_ms\n        }\n\ndef generate_prompts():\n    """Generate all prompts from template and variables"""\n    template = EXPERIMENT["prompt_template"]\n    variables = EXPERIMENT["variables"]\n    \n    # Get variable names from template\n    import re\n    var_names = re.findall(r'{{(\\w+)}}', template)\n    \n    # Generate all combinations\n    from itertools import product\n    \n    var_lists = [variables[var] for var in var_names]\n    for values in product(*var_lists):\n        var_dict = dict(zip(var_names, values))\n        \n        # Replace variables in template\n        prompt = template\n        for var, val in var_dict.items():\n            prompt = prompt.replace(f"{{{{{var}}}}}", str(val))\n        \n        yield prompt, var_dict\n\ndef run_experiment():\n    """Run the full experiment"""\n    results = []\n    total_calls = len(MODELS) * len(list(generate_prompts()))\n    current = 0\n    \n    print(f"Running experiment with {len(MODELS)} models and {total_calls} total API calls")\n    print("=" * 60)\n    \n    for model_config in MODELS:\n        print(f"\\nTesting {model_config['displayName']}...")\n        \n        for prompt, variables in generate_prompts():\n            current += 1\n            print(f"[{current}/{total_calls}] {prompt[:50]}...", end=" ")\n            \n            # Make API call\n            result = make_api_call(model_config, prompt)\n            \n            # Collect results\n            results.append({\n                "timestamp": datetime.now(),\n                "provider": model_config["provider"],\n                "model": model_config["modelId"],\n                "model_name": model_config["displayName"],\n                "prompt": prompt,\n                "response": str(result.get("content", "")),\n                "success": result.get("success", False),\n                "error": result.get("error", ""),\n                "latency_ms": result.get("latency_ms", 0),\n                **variables  # Add variables as columns\n            })\n            \n            # Show result\n            if result["success"]:\n                print(f"✓ {str(result['content'])[:30]}")\n            else:\n                print(f"✗ {result['error'][:30]}")\n            \n            # Rate limiting\n            time.sleep(0.1)\n    \n    return results\n\ndef save_results(results: list, format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_native_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for required packages\n    required = ${JSON.stringify(e.providerLibraries.required)}\n    missing = []\n    for lib in required:\n        try:\n            __import__(lib)\n        except ImportError:\n            missing.append(lib)\n    \n    if missing:\n        print(f"ERROR: Missing required packages: {', '.join(missing)}")\n        print(f"Install with: pip install {' '.join(missing)}")\n        return\n    \n    # Check for API keys\n    missing_keys = []\n    for model in MODELS:\n        provider = model["provider"]\n        if provider in ["openai-chat", "openai-responses"] and not os.environ.get("OPENAI_API_KEY"):\n            missing_keys.append("OPENAI_API_KEY")\n        elif provider == "anthropic" and not os.environ.get("ANTHROPIC_API_KEY"):\n            missing_keys.append("ANTHROPIC_API_KEY")\n        elif provider == "openrouter" and not os.environ.get("OPENROUTER_API_KEY"):\n            missing_keys.append("OPENROUTER_API_KEY")\n    \n    if missing_keys:\n        print(f"WARNING: Missing API keys: {', '.join(set(missing_keys))}")\n        print("Set them in the script or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Run experiment\n    results = run_experiment()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if 'latency_ms' in df.columns and df['success'].any():\n            print(f"Avg latency: {df[df['success']]['latency_ms'].mean():.1f}ms")\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class Ge{static async generatePythonScript(e,n){try{const t=n||this.getDefaultOptions(),a=this.validateTrialForExport(e);if(!a.valid)throw new Error(`Trial validation failed: ${a.errors.join(", ")}`);switch(t.mode){case"simple":return Ke.generate(e);case"literal":return Be.generate(e);case"native":return Ye.generate(e);default:throw new Error(`Unknown export mode: ${t.mode}`)}}catch(t){throw new Error(`Failed to generate Python export: ${t instanceof Error?t.message:String(t)}`)}}static async downloadPythonScript(e,n){const t=await this.generatePythonScript(e,n),a=n||this.getDefaultOptions(),s=new Blob([t],{type:"text/x-python"}),r=URL.createObjectURL(s),l=document.createElement("a");l.href=r,l.download=this.generateFilename(e,a.mode),document.body.appendChild(l),l.click(),document.body.removeChild(l),URL.revokeObjectURL(r)}static validateTrialForExport(e){const n=[];return e.designSnapshot?e.designSnapshot.promptTemplate||n.push("Design missing prompt template"):n.push("Trial missing design snapshot"),e.configurationSnapshots&&0!==e.configurationSnapshots.length?e.configurationSnapshots.forEach((e,t)=>{e.provider||n.push(`Configuration ${t+1} missing provider`),e.modelId||n.push(`Configuration ${t+1} missing model`),e.parameters||n.push(`Configuration ${t+1} missing parameters`)}):n.push("Trial missing model configurations"),e.variableSnapshots||n.push("Trial missing variable snapshots"),{valid:0===n.length,errors:n}}static getExportSummary(e){const n=new Set(e.configurationSnapshots.map(e=>e.provider)),t=e.totalCombinations||0;return{apiCallCount:e.configurationSnapshots.length*t,providersUsed:Array.from(n),variableCombinations:t,configurations:e.configurationSnapshots.length}}static getDefaultOptions(){return{mode:"simple"}}static generateFilename(e,n){const t=e.name||`trial_${e.id}`,a=(new Date).toISOString().split("T")[0];return`${t.toLowerCase().replace(/[^a-z0-9]/g,"_")}_${n}_${a}.py`}}const Je=Object.freeze(Object.defineProperty({__proto__:null,PythonExportService:Ge},Symbol.toStringTag,{value:"Module"})),He={class:"trial-info"},Ve={class:"trial-stats"},Xe={class:"export-section"},We={class:"mode-content"},Ze={class:"mode-content"},Qe={class:"mode-content"},en={class:"export-section"},nn={class:"preview-content"},tn={class:"preview-info"},an=g({__name:"PythonExportModal",props:{trial:{}},emits:["close","exported"],setup(e,{emit:n}){const t=e,a=n,s=_("simple"),r=_(!1),l=y(()=>t.trial.progress.total),o=y(()=>t.trial.configurationSnapshots?.length||0),i=y(()=>t.trial.totalCombinations||0),p=y(()=>{const e=.05*i.value+.3*o.value;return Math.round(15+e)}),c=y(()=>{const e=.5*l.value;return Math.round(10+e)}),d=y(()=>{const e=.05*i.value+.2*o.value;return Math.round(12+e)}),m=y(()=>{const e=t.trial.name.toLowerCase().replace(/\s+/g,"_"),n=(new Date).toISOString().split("T")[0];return`${e}_${s.value}_${n}.py`}),u=y(()=>{if("simple"===s.value){return 300+(i.value+10*o.value)}if("native"===s.value){return 250+(i.value+8*o.value)}return 200+15*l.value});async function f(){r.value=!0;try{const e={mode:s.value};await Ge.downloadPythonScript(t.trial,e),a("exported",m.value),a("close")}catch(e){console.error("Export failed:",e),alert("Export failed: "+(e instanceof Error?e.message:"Unknown error"))}finally{r.value=!1}}return(e,n)=>{const t=x("a-button"),a=x("a-tag"),v=x("a-radio"),g=x("a-radio-group"),_=x("a-typography-text"),y=x("a-modal");return C(),k(y,{open:!0,title:"Export Python Script",width:"95vw",centered:!0,onCancel:n[2]||(n[2]=n=>e.$emit("close")),"wrap-class-name":"python-export-modal","body-style":{height:"90vh",overflow:"auto"}},{footer:P(()=>[S(t,{onClick:n[0]||(n[0]=n=>e.$emit("close")),size:"large"},{default:P(()=>n[3]||(n[3]=[O(" Cancel ")])),_:1,__:[3]}),S(t,{type:"primary",onClick:f,loading:r.value,size:"large"},{default:P(()=>n[4]||(n[4]=[O(" Export Script ")])),_:1,__:[4]},8,["loading"])]),default:P(()=>[w("div",He,[w("h3",null,T(e.trial.name),1),w("div",Ve,[S(a,null,{default:P(()=>[O(T(l.value)+" API calls",1)]),_:1}),S(a,null,{default:P(()=>[O(T(o.value)+" configurations",1)]),_:1}),S(a,null,{default:P(()=>[O(T(i.value)+" variable combinations",1)]),_:1})])]),w("div",Xe,[n[11]||(n[11]=w("h4",null,"Export Mode",-1)),S(g,{value:s.value,"onUpdate:value":n[1]||(n[1]=e=>s.value=e),class:"mode-options"},{default:P(()=>[S(v,{value:"simple",class:"mode-radio"},{default:P(()=>[w("div",We,[n[5]||(n[5]=w("div",{class:"mode-title"},"Simple Script",-1)),n[6]||(n[6]=w("div",{class:"mode-description"}," Educational script with variables as lists. Easy to understand, modify, and extend. Perfect for learning how AI APIs work. ",-1)),S(a,{color:"blue",size:"small"},{default:P(()=>[O("~"+T(p.value)+"KB",1)]),_:1})])]),_:1}),S(v,{value:"literal",class:"mode-radio"},{default:P(()=>[w("div",Ze,[n[7]||(n[7]=w("div",{class:"mode-title"},"Literal Reproduction",-1)),n[8]||(n[8]=w("div",{class:"mode-description"}," Exact API calls pre-computed. Bit-for-bit reproduction of your experiment. Best for debugging and comparing results. ",-1)),S(a,{color:"blue",size:"small"},{default:P(()=>[O("~"+T(c.value)+"KB",1)]),_:1})])]),_:1}),S(v,{value:"native",class:"mode-radio"},{default:P(()=>[w("div",Qe,[n[9]||(n[9]=w("div",{class:"mode-title"},"Native Libraries",-1)),n[10]||(n[10]=w("div",{class:"mode-description"}," Uses official Python SDKs (openai, anthropic, ollama). Cleanest code, best for production use. Requires: pip install openai anthropic ollama ",-1)),S(a,{color:"green",size:"small"},{default:P(()=>[O("~"+T(d.value)+"KB",1)]),_:1})])]),_:1})]),_:1},8,["value"])]),n[13]||(n[13]=w("div",{class:"export-section"},[w("h4",null,"Output Format"),w("div",{class:"format-info"},[w("p",null,"Both scripts save results using pandas in your choice of format:"),w("ul",null,[w("li",null,[w("strong",null,"CSV"),O(" - Universal format, opens in Excel/Google Sheets")]),w("li",null,[w("strong",null,"Excel"),O(" - Native Excel format")]),w("li",null,[w("strong",null,"JSON"),O(" - For programmatic access")]),w("li",null,[w("strong",null,"Parquet"),O(" - Efficient compressed format")]),w("li",null,[w("strong",null,"HTML"),O(" - For web viewing")]),w("li",null,[w("strong",null,"Markdown"),O(" - For documentation")]),w("li",null,[w("strong",null,"Stata"),O(" - For statistical analysis")]),w("li",null,[w("strong",null,"Pickle"),O(" - Python native format")])])])],-1)),w("div",en,[n[12]||(n[12]=w("h4",null,"Script Preview",-1)),w("div",nn,[S(_,{code:"",class:"preview-filename"},{default:P(()=>[O(T(m.value),1)]),_:1}),w("div",tn,[S(a,{size:"small"},{default:P(()=>[O(T(u.value)+" lines",1)]),_:1}),S(a,{size:"small"},{default:P(()=>[O(T(s.value)+" mode",1)]),_:1})])])])]),_:1,__:[13]})}}}),sn={class:"api-call-modal"},rn={class:"modal-header"},ln={class:"modal-content"},on={class:"section"},pn={class:"info-grid"},cn={class:"info-item"},dn={class:"call-id"},mn={class:"info-item"},un={class:"info-item"},fn={class:"info-item"},vn={key:0,class:"info-item"},gn={key:1,class:"info-item"},_n={key:2,class:"info-item"},yn={class:"section"},hn={class:"variables-detail"},bn={class:"variable-value"},kn={key:0,class:"attributes-section"},xn={class:"attribute-items"},Pn={class:"section"},Cn={class:"prompt-display"},wn={key:0,class:"section"},Sn={key:0,class:"response-info"},In={class:"info-grid"},An={class:"info-item"},En={class:"info-item"},On={key:1,class:"result-content"},Tn={key:0,class:"error-result"},Nn={class:"error-message"},Mn={key:1,class:"content-result"},Rn={class:"content-display"},jn={class:"section"},$n={class:"raw-data"},Fn={key:1,class:"section"},Dn={class:"raw-data"},Un={class:"modal-footer"},Ln=f(g({__name:"APICallDetailModal",props:{apiCall:{},trial:{}},emits:["close"],setup(e){const n=e,t=y(()=>{if(!n.apiCall.request)return"No request data";const e=JSON.parse(JSON.stringify(n.apiCall.request));return e.headers&&Object.keys(e.headers).forEach(n=>{const t=n.toLowerCase();(t.includes("authorization")||t.includes("api-key")||t.includes("x-api-key")||t.includes("bearer"))&&(e.headers[n]="[REDACTED]")}),JSON.stringify(e,null,2)});function a(){return n.trial&&n.trial.configurationSnapshots[n.apiCall.configurationIndex]&&n.trial.configurationSnapshots[n.apiCall.configurationIndex].name||`Configuration ${n.apiCall.configurationIndex+1}`}function s(e){const n="string"==typeof e?new Date(e):e;return isNaN(n.getTime())?"Invalid date":n.toLocaleString()}async function r(){const e={id:n.apiCall.id,status:n.apiCall.status,configuration:a(),variables:n.apiCall.variables,variableAttributes:n.apiCall.variableAttributes,prompt:n.apiCall.prompt,request:JSON.parse(t.value),response:n.apiCall.response,result:n.apiCall.result,created:n.apiCall.created,completed:n.apiCall.completed},s=JSON.stringify(e,null,2);try{if(navigator.clipboard&&navigator.clipboard.writeText)return await navigator.clipboard.writeText(s),void v.success("Details copied to clipboard!");const e=document.createElement("textarea");e.value=s,e.style.position="fixed",e.style.left="-999999px",e.style.top="-999999px",document.body.appendChild(e),e.focus(),e.select();const n=document.execCommand("copy");if(document.body.removeChild(e),!n)throw new Error("execCommand failed");v.success("Details copied to clipboard!")}catch(r){console.error("Failed to copy to clipboard:",r),prompt("Copy this text manually:",s)}}return(e,n)=>{const l=x("a-button");return C(),I("div",{class:"modal-overlay",onClick:n[2]||(n[2]=R(n=>e.$emit("close"),["self"]))},[w("div",sn,[w("div",rn,[n[3]||(n[3]=w("h2",null,"API Call Details",-1)),w("button",{class:"close-btn",onClick:n[0]||(n[0]=n=>e.$emit("close"))},"×")]),w("div",ln,[w("div",on,[n[11]||(n[11]=w("h3",null,"Overview",-1)),w("div",pn,[w("div",cn,[n[4]||(n[4]=w("label",null,"Call ID:",-1)),w("span",dn,T(e.apiCall.id),1)]),w("div",mn,[n[5]||(n[5]=w("label",null,"Status:",-1)),w("span",{class:j(["status-badge",e.apiCall.status])},T(e.apiCall.status),3)]),w("div",un,[n[6]||(n[6]=w("label",null,"Configuration:",-1)),w("span",null,T(a()),1)]),w("div",fn,[n[7]||(n[7]=w("label",null,"Created:",-1)),w("span",null,T(s(e.apiCall.created)),1)]),e.apiCall.completed?(C(),I("div",vn,[n[8]||(n[8]=w("label",null,"Completed:",-1)),w("span",null,T(s(e.apiCall.completed)),1)])):A("",!0),e.apiCall.completed?(C(),I("div",gn,[n[9]||(n[9]=w("label",null,"Duration:",-1)),w("span",null,T((o=e.apiCall.completed.getTime()-e.apiCall.created.getTime(),o<1e3?`${o}ms`:`${(o/1e3).toFixed(1)}s`)),1)])):A("",!0),e.apiCall.response?.latencyMs?(C(),I("div",_n,[n[10]||(n[10]=w("label",null,"API Latency:",-1)),w("span",null,T(e.apiCall.response.latencyMs)+"ms",1)])):A("",!0)])]),w("div",yn,[n[13]||(n[13]=w("h3",null,"Variables",-1)),w("div",hn,[(C(!0),I(N,null,E(Object.entries(e.apiCall.variables),([e,n])=>(C(),I("div",{key:e,class:"variable-item"},[w("label",null,T(e)+":",1),w("span",bn,T(n),1)]))),128))]),e.apiCall.variableAttributes&&Object.keys(e.apiCall.variableAttributes).length>0?(C(),I("div",kn,[n[12]||(n[12]=w("h4",null,"Variable Attributes",-1)),(C(!0),I(N,null,E(Object.entries(e.apiCall.variableAttributes),([e,n])=>(C(),I("div",{key:e,class:"attribute-group"},[w("h5",null,T(e),1),w("div",xn,[(C(!0),I(N,null,E(Object.entries(n),([e,n])=>(C(),I("div",{key:e,class:"attribute-item"},[w("label",null,T(e)+":",1),w("span",null,T(n),1)]))),128))])]))),128))])):A("",!0)]),w("div",Pn,[n[14]||(n[14]=w("h3",null,"Resolved Prompt",-1)),w("div",Cn,T(e.apiCall.prompt),1)]),e.apiCall.response||e.apiCall.result?(C(),I("div",wn,[n[19]||(n[19]=w("h3",null,"Response",-1)),e.apiCall.response?(C(),I("div",Sn,[w("div",In,[w("div",An,[n[15]||(n[15]=w("label",null,"HTTP Status:",-1)),w("span",null,T(e.apiCall.response.status),1)]),w("div",En,[n[16]||(n[16]=w("label",null,"Latency:",-1)),w("span",null,T(e.apiCall.response.latencyMs)+"ms",1)])])])):A("",!0),e.apiCall.result?(C(),I("div",On,[!1===e.apiCall.result.success?(C(),I("div",Tn,[n[17]||(n[17]=w("h4",null,"Error",-1)),w("div",Nn,T(e.apiCall.result.error),1)])):A("",!0),e.apiCall.result.content?(C(),I("div",Mn,[n[18]||(n[18]=w("h4",null,"Content",-1)),w("div",Rn,T(e.apiCall.result.content),1)])):A("",!0)])):A("",!0)])):A("",!0),w("div",jn,[n[20]||(n[20]=w("h3",null,"Raw Request",-1)),w("pre",$n,T(t.value),1)]),e.apiCall.response?(C(),I("div",Fn,[n[21]||(n[21]=w("h3",null,"Raw Response",-1)),w("pre",Dn,T(JSON.stringify(e.apiCall.response,null,2)),1)])):A("",!0)]),w("div",Un,[S(l,{onClick:n[1]||(n[1]=n=>e.$emit("close")),size:"large",class:"footer-button"},{default:P(()=>n[22]||(n[22]=[O(" Close ")])),_:1,__:[22]}),S(l,{type:"primary",onClick:r,size:"large",class:"footer-button footer-button-primary"},{default:P(()=>n[23]||(n[23]=[O(" Copy Details ")])),_:1,__:[23]})])])]);var o}}}),[["__scopeId","data-v-595c884b"]]);export{Ln as A,Oe as T,ze as _,an as a,Je as p};
