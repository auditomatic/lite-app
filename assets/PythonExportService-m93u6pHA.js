const __vite__mapDeps=(i,m=__vite__mapDeps,d=(m.f||(m.f=["./jszip.min-D2NhGjAT.js","./db-ftwlfnxb.js"])))=>i.map(i=>d[i]);
var e=Object.defineProperty,n=(n,t,r)=>((n,t,r)=>t in n?e(n,t,{enumerable:!0,configurable:!0,writable:!0,value:r}):n[t]=r)(n,"symbol"!=typeof t?t+"":t,r);import{c as t,D as r}from"./index-niVOvZdn.js";import"./vendor-DdqHZeTC.js";import"./db-ftwlfnxb.js";const a=class e{constructor(){}static getInstance(){return e.instance||(e.instance=new e),e.instance}generateMainScript(e,n){const t=this.generateProviderFunctions();return`#!/usr/bin/env python3\n"""\nAuditomatic Lite Reproduction Script\nGenerated on: ${e.metadata.exportedAt}\nRun: ${e.run.name}\nDesign: ${e.design.name}\n"""\n\nimport json\nimport os\nimport sys\nimport time\nimport logging\nfrom typing import Dict, List, Any, Optional\nfrom itertools import product\nfrom pathlib import Path\nimport requests\nfrom dotenv import load_dotenv\n${n.enableProgressBar?"from tqdm import tqdm":""}\n${"csv"===n.outputFormat||"both"===n.outputFormat?"import pandas as pd":""}\n\n# Load environment variables\nload_dotenv()\n\n# Configuration\nCONFIG_DIR = Path("config")\nOUTPUT_DIR = Path("output")\nMAX_RETRIES = ${n.maxRetries}\nRETRY_BACKOFF = ${n.retryBackoffMultiplier}\nCONTINUE_ON_ERROR = ${n.continueOnError?"True":"False"}\nTIMEOUT_OVERRIDE = ${n.timeoutOverride||"None"}\nVERBOSE_LOGGING = ${n.verboseLogging?"True":"False"}\nENABLE_CHECKPOINTING = ${n.enableCheckpointing?"True":"False"}\n\ndef setup_logging():\n    """Setup logging configuration"""\n    level = logging.DEBUG if VERBOSE_LOGGING else logging.INFO\n    logging.basicConfig(\n        level=level,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(OUTPUT_DIR / 'execution.log'),\n            logging.StreamHandler()\n        ]\n    )\n    return logging.getLogger(__name__)\n\ndef load_config_files():\n    """Load all configuration files"""\n    config = {}\n    config_files = ['run_config.json', 'design.json', 'models.json', 'providers.json', 'variable_lists.json']\n    \n    for filename in config_files:\n        file_path = CONFIG_DIR / filename\n        with open(file_path, 'r') as f:\n            key = filename.replace('.json', '').replace('_config', '')\n            config[key] = json.load(f)\n    \n    return config\n\ndef get_api_key(provider_id: str) -> Optional[str]:\n    """Get API key for provider from environment variables"""\n    key_map = {\n        'openai': 'OPENAI_API_KEY',\n        'anthropic': 'ANTHROPIC_API_KEY',\n        'openrouter': 'OPENROUTER_API_KEY',\n        'ollama': None  # Ollama typically doesn't require API key\n    }\n    \n    env_var = key_map.get(provider_id)\n    if env_var:\n        return os.getenv(env_var)\n    return None\n\n${t}\n\ndef generate_variable_combinations(design: Dict, variable_lists: List[Dict]) -> List[Dict[str, str]]:\n    """Generate all variable combinations for the design"""\n    all_variables = {}\n    \n    # Add simple variables from design\n    for var_name, values in design.get('variables', {}).items():\n        all_variables[var_name] = values if isinstance(values, list) else [values]\n    \n    # Add variables from variable lists\n    variable_list_map = {vl['id']: vl for vl in variable_lists}\n    for var_name, list_id in design.get('variable_list_refs', {}).items():\n        if list_id in variable_list_map:\n            all_variables[var_name] = variable_list_map[list_id]['values']\n    \n    if not all_variables:\n        return [{}]\n    \n    # Generate all combinations\n    var_names = list(all_variables.keys())\n    var_values = [all_variables[name] for name in var_names]\n    \n    combinations = []\n    for combination in product(*var_values):\n        combinations.append(dict(zip(var_names, combination)))\n    \n    return combinations\n\ndef apply_variables_to_prompt(template: str, variables: Dict[str, str]) -> str:\n    """Apply variables to prompt template"""\n    prompt = template\n    for var_name, value in variables.items():\n        prompt = prompt.replace(f'{{{{{var_name}}}}}', value)\n    return prompt\n\ndef execute_task(task: Dict, providers: Dict, logger: logging.Logger) -> Dict:\n    """Execute a single task"""\n    model_id = task['model']\n    prompt = task['prompt']\n    variables = task['variables']\n    design = providers['design']\n    \n    # Find model and provider\n    models = providers['models']\n    model = next((m for m in models if m['id'] == model_id), None)\n    if not model:\n        raise ValueError(f"Model {model_id} not found")\n    \n    provider_id = model['provider']\n    provider_config = next((p for p in providers['providers'] if p['id'] == provider_id), None)\n    if not provider_config:\n        raise ValueError(f"Provider {provider_id} not found")\n    \n    # Get API key\n    api_key = get_api_key(provider_id)\n    \n    # Determine output format and type from design\n    output_format = design.get('output_format', 'text')\n    output_type = design.get('output_type', 'string')\n    \n    # Execute based on provider\n    start_time = time.time()\n    \n    # Get run parameters\n    run_params = providers['run']['parameters']\n    \n    if provider_id == 'openai':\n        result = call_openai(prompt, model_id, api_key, provider_config, output_format, output_type, run_params)\n    elif provider_id == 'anthropic':\n        result = call_anthropic(prompt, model_id, api_key, provider_config, output_format, output_type, run_params)\n    elif provider_id == 'ollama':\n        result = call_ollama(prompt, model_id, provider_config, output_format, output_type, run_params)\n    elif provider_id == 'openrouter':\n        result = call_openrouter(prompt, model_id, api_key, provider_config, output_format, output_type, run_params)\n    else:\n        raise ValueError(f"Unsupported provider: {provider_id}")\n    \n    response_time = time.time() - start_time\n    \n    # Apply extraction pattern if specified\n    raw_content = result.get('content', '')\n    extracted_value = extract_value_from_response(raw_content, design)\n    \n    return {\n        'model': model_id,\n        'prompt': prompt,\n        'variables': variables,\n        'result': raw_content,\n        'extracted_value': extracted_value,\n        'usage': result.get('usage'),\n        'response_time': response_time,\n        'status': 'completed'\n    }\n\ndef extract_value_from_response(content: str, design: Dict) -> str:\n    """Extract value from response using design's extraction pattern"""\n    import re\n    \n    # If no extraction pattern specified, return full content\n    extract_pattern = design.get('extract_pattern')\n    if not extract_pattern:\n        return content\n    \n    try:\n        if isinstance(extract_pattern, dict):\n            # Complex pattern with match and replace\n            match_pattern = extract_pattern.get('match', '')\n            replace_pattern = extract_pattern.get('replace', '')\n            \n            if match_pattern:\n                match = re.search(match_pattern, content, re.IGNORECASE | re.DOTALL)\n                if match:\n                    if replace_pattern:\n                        return match.expand(replace_pattern)\n                    else:\n                        return match.group(1) if match.groups() else match.group(0)\n        else:\n            # Simple regex pattern\n            match = re.search(str(extract_pattern), content, re.IGNORECASE | re.DOTALL)\n            if match:\n                return match.group(1) if match.groups() else match.group(0)\n    except Exception as e:\n        print(f"Warning: Extraction pattern failed: {e}")\n    \n    return content\n\ndef save_results(results: List[Dict], config: Dict):\n    """Save results in specified format(s)"""\n    OUTPUT_DIR.mkdir(exist_ok=True)\n    \n    ${"json"===n.outputFormat||"both"===n.outputFormat?"\n    # Save as JSON\n    with open(OUTPUT_DIR / 'results.json', 'w') as f:\n        json.dump(results, f, indent=2)\n    ":""}\n    \n    ${"csv"===n.outputFormat||"both"===n.outputFormat?"\n    # Save as CSV\n    df = pd.DataFrame(results)\n    df.to_csv(OUTPUT_DIR / 'results.csv', index=False)\n    ":""}\n\ndef main():\n    """Main execution function"""\n    logger = setup_logging()\n    logger.info("Starting reproduction script")\n    \n    # Load configuration\n    config = load_config_files()\n    \n    # Generate tasks\n    combinations = generate_variable_combinations(config['design'], config['variable_lists'])\n    tasks = []\n    \n    for combination in combinations:\n        prompt = apply_variables_to_prompt(config['design']['prompt_template'], combination)\n        for model_id in config['run']['models']:\n            for rep in range(config['run']['parameters']['repetitions_per_prompt']):\n                tasks.append({\n                    'model': model_id,\n                    'prompt': prompt,\n                    'variables': combination,\n                    'repetition': rep + 1\n                })\n    \n    logger.info(f"Generated {len(tasks)} tasks")\n    \n    # Execute tasks\n    results = []\n    ${n.enableProgressBar?'progress_bar = tqdm(tasks, desc="Executing tasks")':""}\n    \n    for i, task in enumerate(${n.enableProgressBar?"progress_bar":"tasks"}):\n        try:\n            result = execute_task(task, config, logger)\n            results.append(result)\n            ${n.enableProgressBar?"":'logger.info(f"Completed task {i+1}/{len(tasks)}")'}\n        except Exception as e:\n            error_msg = f"Task {i+1} failed: {str(e)}"\n            logger.error(error_msg)\n            \n            if CONTINUE_ON_ERROR:\n                results.append({\n                    **task,\n                    'result': '',\n                    'status': 'error',\n                    'error': str(e)\n                })\n                continue\n            else:\n                logger.error("Stopping execution due to error")\n                break\n    \n    # Save results\n    save_results(results, config)\n    \n    # Generate summary\n    completed_tasks = len([r for r in results if r.get('status') == 'completed'])\n    failed_tasks = len([r for r in results if r.get('status') == 'error'])\n    \n    summary = {\n        'total_tasks': len(tasks),\n        'completed_tasks': completed_tasks,\n        'failed_tasks': failed_tasks,\n        'completion_rate': completed_tasks / len(tasks) if tasks else 0,\n        'execution_time': time.time(),\n        'config': config['run']\n    }\n    \n    with open(OUTPUT_DIR / 'summary.json', 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    logger.info(f"Execution complete: {completed_tasks}/{len(tasks)} tasks completed")\n\nif __name__ == "__main__":\n    main()\n`}generateProviderFunctions(){return"\ndef call_openai(prompt: str, model: str, api_key: str, provider_config: Dict, output_format: str = 'text', output_type: str = 'string', run_params: Dict = None) -> Dict:\n    \"\"\"Call OpenAI API\"\"\"\n    headers = {\n        'Authorization': f'Bearer {api_key}',\n        'Content-Type': 'application/json'\n    }\n    \n    if run_params is None:\n        run_params = {}\n    \n    data = {\n        'model': model,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': run_params.get('temperature', 0.7),\n        'max_tokens': run_params.get('max_tokens', 1000)\n    }\n    \n    if output_format == 'json':\n        answer_type = 'number' if output_type == 'number' else 'string'\n        data['functions'] = [{\n            'name': 'provide_answer',\n            'description': 'Provides an answer to the prompt',\n            'parameters': {\n                'type': 'object',\n                'properties': {\n                    'answer': {\n                        'type': answer_type,\n                        'description': 'The answer value'\n                    }\n                },\n                'required': ['answer']\n            }\n        }]\n        data['function_call'] = {'name': 'provide_answer'}\n    \n    timeout = TIMEOUT_OVERRIDE or provider_config.get('timeout_ms', 60000) / 1000\n    \n    response = requests.post(\n        'https://api.openai.com/v1/chat/completions',\n        headers=headers,\n        json=data,\n        timeout=timeout\n    )\n    response.raise_for_status()\n    \n    result = response.json()\n    message = result['choices'][0]['message']\n    \n    content = ''\n    if message.get('function_call'):\n        try:\n            args = json.loads(message['function_call']['arguments'])\n            content = str(args.get('answer', ''))\n        except json.JSONDecodeError:\n            content = message['function_call']['arguments']\n    else:\n        content = message.get('content', '')\n    \n    return {\n        'content': content,\n        'usage': result.get('usage')\n    }\n\ndef call_anthropic(prompt: str, model: str, api_key: str, provider_config: Dict, output_format: str = 'text', output_type: str = 'string', run_params: Dict = None) -> Dict:\n    \"\"\"Call Anthropic API\"\"\"\n    headers = {\n        'x-api-key': api_key,\n        'Content-Type': 'application/json',\n        'anthropic-version': '2023-06-01'\n    }\n    \n    if run_params is None:\n        run_params = {}\n    \n    data = {\n        'model': model,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'max_tokens': run_params.get('max_tokens', 1000),\n        'temperature': run_params.get('temperature', 0.7)\n    }\n    \n    if output_format == 'json':\n        answer_type = 'number' if output_type == 'number' else 'string'\n        headers['anthropic-beta'] = 'tools-2024-04-04'\n        data['tools'] = [{\n            'name': 'provide_answer',\n            'description': 'Provides an answer to the prompt',\n            'input_schema': {\n                'type': 'object',\n                'properties': {\n                    'answer': {\n                        'type': answer_type,\n                        'description': 'The answer value'\n                    }\n                },\n                'required': ['answer']\n            }\n        }]\n        data['tool_choice'] = {'type': 'tool', 'name': 'provide_answer'}\n    \n    timeout = TIMEOUT_OVERRIDE or provider_config.get('timeout_ms', 60000) / 1000\n    \n    response = requests.post(\n        'https://api.anthropic.com/v1/messages',\n        headers=headers,\n        json=data,\n        timeout=timeout\n    )\n    response.raise_for_status()\n    \n    result = response.json()\n    content = ''\n    \n    if result.get('content'):\n        for item in result['content']:\n            if item.get('type') == 'tool_use' and item.get('input'):\n                content = str(item['input'].get('answer', ''))\n                break\n            elif item.get('type') == 'text':\n                content += item.get('text', '')\n    \n    return {\n        'content': content,\n        'usage': result.get('usage')\n    }\n\ndef call_ollama(prompt: str, model: str, provider_config: Dict, output_format: str = 'text', output_type: str = 'string', run_params: Dict = None) -> Dict:\n    \"\"\"Call Ollama API\"\"\"\n    model_name = model.replace('ollama:', '') if model.startswith('ollama:') else model\n    \n    data = {\n        'model': model_name,\n        'prompt': prompt,\n        'stream': False\n    }\n    \n    if output_format == 'json':\n        answer_type = 'number' if output_type == 'number' else 'string'\n        data['format'] = {\n            'type': 'object',\n            'properties': {\n                'answer': {'type': answer_type}\n            },\n            'required': ['answer']\n        }\n        data['prompt'] = prompt + '\\\\n\\\\nPlease respond with valid JSON in the format: {\"answer\": <your_answer>}'\n    \n    base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n    timeout = TIMEOUT_OVERRIDE or provider_config.get('timeout_ms', 60000) / 1000\n    \n    response = requests.post(\n        f'{base_url}/api/generate',\n        json=data,\n        timeout=timeout\n    )\n    response.raise_for_status()\n    \n    result = response.json()\n    content = result.get('response', '')\n    \n    if output_format == 'json':\n        try:\n            json_response = json.loads(content)\n            if isinstance(json_response, dict) and 'answer' in json_response:\n                content = str(json_response['answer'])\n        except json.JSONDecodeError:\n            pass\n    \n    return {\n        'content': content,\n        'usage': None\n    }\n\ndef call_openrouter(prompt: str, model: str, api_key: str, provider_config: Dict, output_format: str = 'text', output_type: str = 'string', run_params: Dict = None) -> Dict:\n    \"\"\"Call OpenRouter API\"\"\"\n    headers = {\n        'Authorization': f'Bearer {api_key}',\n        'Content-Type': 'application/json',\n        'HTTP-Referer': 'https://lite.auditomatic.org',\n        'X-Title': 'Auditomatic Lite Export'\n    }\n    \n    if run_params is None:\n        run_params = {}\n    \n    data = {\n        'model': model,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': run_params.get('temperature', 0.7),\n        'max_tokens': run_params.get('max_tokens', 1000)\n    }\n    \n    if output_format == 'json':\n        answer_type = 'number' if output_type == 'number' else 'string'\n        data['functions'] = [{\n            'name': 'provide_answer',\n            'description': 'Provides an answer to the prompt',\n            'parameters': {\n                'type': 'object',\n                'properties': {\n                    'answer': {\n                        'type': answer_type,\n                        'description': 'The answer value'\n                    }\n                },\n                'required': ['answer']\n            }\n        }]\n        data['function_call'] = {'name': 'provide_answer'}\n    \n    timeout = TIMEOUT_OVERRIDE or provider_config.get('timeout_ms', 60000) / 1000\n    \n    response = requests.post(\n        'https://openrouter.ai/api/v1/chat/completions',\n        headers=headers,\n        json=data,\n        timeout=timeout\n    )\n    response.raise_for_status()\n    \n    result = response.json()\n    message = result['choices'][0]['message']\n    \n    content = ''\n    if message.get('function_call'):\n        try:\n            args = json.loads(message['function_call']['arguments'])\n            content = str(args.get('answer', ''))\n        except json.JSONDecodeError:\n            content = message['function_call']['arguments']\n    else:\n        content = message.get('content', '')\n    \n    return {\n        'content': content,\n        'usage': result.get('usage')\n    }\n"}};n(a,"instance");let s=a;const o=class e{constructor(){}static getInstance(){return e.instance||(e.instance=new e),e.instance}generateReadme(e,n){return`# ${e.run.name} - Reproduction Package\n\nGenerated from Auditomatic Lite on ${e.metadata.exportedAt}\n\n## Overview\n\nThis package contains everything needed to reproduce the run "${e.run.name}" using the design "${e.design.name}".\n\n## Setup\n\n1. Install Python ${n.pythonVersion}\n2. Install dependencies:\n   \`\`\`bash\n   pip install -r requirements.txt\n   \`\`\`\n\n3. Configure API keys by copying \`.env.template\` to \`.env\` and filling in your keys:\n   \`\`\`bash\n   cp .env.template .env\n   # Edit .env with your API keys\n   \`\`\`\n\n## Running\n\nExecute the reproduction script:\n\n\`\`\`bash\npython run.py\n\`\`\`\n\nResults will be saved to the \`output/\` directory in ${"both"===n.outputFormat?"both JSON and CSV formats":n.outputFormat.toUpperCase()+" format"}.\n\n## Configuration\n\n- **Models**: ${e.models.map(e=>e.name).join(", ")}\n- **Total Tasks**: ${this.calculateTaskCount(e)}\n- **Parameters**: \n  - Temperature: ${e.run.parameters.temperature}\n  - Max Tokens: ${e.run.parameters.max_tokens}\n  - Concurrency: ${e.run.parameters.concurrency}\n  - Repetitions: ${e.run.parameters.repetitions_per_prompt}\n\n## Files\n\n- \`run.py\` - Main execution script\n- \`config/\` - Configuration files (design, models, providers, variables)\n- \`output/\` - Results directory (created during execution)\n- \`.env.template\` - Environment variables template\n\n## Export Options\n\n- Progress bar: ${n.enableProgressBar?"Enabled":"Disabled"}\n- Verbose logging: ${n.verboseLogging?"Enabled":"Disabled"}\n- Checkpointing: ${n.enableCheckpointing?"Enabled":"Disabled"}\n- Continue on error: ${n.continueOnError?"Yes":"No"}\n- Max retries: ${n.maxRetries}\n- Retry backoff: ${n.retryBackoffMultiplier}x\n${n.timeoutOverride?`- Timeout override: ${n.timeoutOverride}s`:""}\n\nGenerated by Auditomatic Lite v${e.metadata.audiotomaticVersion}\n`}generateEnvTemplate(e){const n=[...new Set(e.models.map(e=>e.provider))];let t="# API Keys for Auditomatic Lite Export\n# Copy this file to .env and fill in your actual API keys\n\n";return n.includes("openai")&&(t+="OPENAI_API_KEY=your_openai_api_key_here\n"),n.includes("anthropic")&&(t+="ANTHROPIC_API_KEY=your_anthropic_api_key_here\n"),n.includes("openrouter")&&(t+="OPENROUTER_API_KEY=your_openrouter_api_key_here\n"),n.includes("ollama")&&(t+="OLLAMA_BASE_URL=http://localhost:11434\n"),t+=`\n# Optional: Override default settings\n# EXPORT_FORMAT=${e.metadata.pythonRequirements.some(e=>e.includes("pandas"))?"both":"json"}\n# MAX_CONCURRENT_REQUESTS=${e.run.parameters.concurrency}\n# ENABLE_DETAILED_LOGGING=true\n`,t}calculateTaskCount(e){let n=1;for(const t of Object.values(e.design.variables))n*=Array.isArray(t)?t.length:1;for(const t of Object.values(e.design.variable_list_refs)){const r=e.variableLists.find(e=>e.id===t);r?.values&&(n*=r.values.length)}return n*e.models.length*e.run.parameters.repetitions_per_prompt}};n(o,"instance");let i=o;const p=class e{constructor(){n(this,"codeGenerator"),n(this,"docGenerator"),this.codeGenerator=s.getInstance(),this.docGenerator=i.getInstance()}static getInstance(){return e.instance||(e.instance=new e),e.instance}async generatePackageFiles(e,n){const t={};return t["run.py"]=this.codeGenerator.generateMainScript(e,n),t["requirements.txt"]=e.metadata.pythonRequirements.join("\n"),t["config/run_config.json"]=JSON.stringify(e.run,null,2),t["config/design.json"]=JSON.stringify(e.design,null,2),t["config/models.json"]=JSON.stringify(e.models,null,2),t["config/providers.json"]=JSON.stringify(e.providers,null,2),t["config/variable_lists.json"]=JSON.stringify(e.variableLists,null,2),t["README.md"]=this.docGenerator.generateReadme(e,n),t[".env.template"]=this.docGenerator.generateEnvTemplate(e),t}async createZipPackage(e){const n=new(0,(await t(async()=>{const{default:e}=await import("./jszip.min-D2NhGjAT.js").then(e=>e.j);return{default:e}},__vite__mapDeps([0,1]),import.meta.url)).default);for(const[t,r]of Object.entries(e))if(t.includes("/")){const e=t.split("/"),a=e.pop(),s=e.join("/"),o=n.folder(s);o&&o.file(a,r)}else n.file(t,r);return await n.generateAsync({type:"blob"})}generatePackageName(e){const n=(new Date).toISOString().slice(0,19).replace(/[:-]/g,"");return`export-${e.replace(/[^a-zA-Z0-9]/g,"_").toLowerCase()}-${n}.zip`}generateRequirements(e){const n=["requests>=2.31.0","python-dotenv>=1.0.0"];return"csv"!==e.outputFormat&&"both"!==e.outputFormat||n.push("pandas>=2.0.0"),e.enableProgressBar&&n.push("tqdm>=4.65.0"),n}};n(p,"instance");let l=p;const u=class e{constructor(){n(this,"dbService"),n(this,"packageBuilder"),this.dbService=r.getInstance(),this.packageBuilder=l.getInstance()}static getInstance(){return e.instance||(e.instance=new e),e.instance}async exportRunConfiguration(e,n){try{if(!e.design_id)return{success:!1,filename:"",size:0,error:"Design ID is required for export"};const t=await this.collectExportData(e,n),r=await this.packageBuilder.generatePackageFiles(t,n),a=await this.packageBuilder.createZipPackage(r);return{success:!0,blob:a,filename:this.packageBuilder.generatePackageName(e.name),size:a.size}}catch(t){return{success:!1,filename:"",size:0,error:t instanceof Error?t.message:"Unknown export error"}}}async collectExportData(e,n){const t=await this.dbService.read("designs",e.design_id);if(!t.success||!t.data)throw new Error("Failed to load design data");const r=t.data,a=Object.values(r.variable_list_refs||{}).map(e=>Number(e)),s=[];for(const c of a){const e=await this.dbService.read("variable_lists",c);e.success&&e.data&&s.push({id:e.data.id,name:e.data.name,description:e.data.description,values:e.data.values,category:e.data.category,attributed_items:e.data.attributed_items})}const o=await this.dbService.findAll("models");if(!o.success||!o.data)throw new Error("Failed to load models data");const i=o.data.filter(n=>e.models.includes(n.id)).map(e=>({id:e.id,name:e.name,provider:e.provider,max_input_tokens:e.max_input_tokens,max_output_tokens:e.max_output_tokens,supports_function_calling:e.supports_function_calling,supports_json_mode:e.supports_json_mode})),p=[...new Set(i.map(e=>e.provider))],l=await this.dbService.findAll("providers"),u=[];if(l.success&&l.data)for(const c of p){const e=l.data.find(e=>e.id===c);e&&u.push(this.sanitizeProviderData(e))}return{run:{name:e.name,design_id:e.design_id,models:e.models,parameters:e.parameters},design:{id:r.id,name:r.name,description:r.description,prompt_template:r.prompt_template,variables:r.variables||{},variable_list_refs:Object.fromEntries(Object.entries(r.variable_list_refs||{}).map(([e,n])=>[e,Number(n)])),output_format:r.output_format,extract_pattern:r.extract_pattern,refusal_words:r.refusal_words},variableLists:s,models:i,providers:u,metadata:{exportedAt:(new Date).toISOString(),audiotomaticVersion:"1.0.0",packageVersion:"1.0.0",pythonRequirements:this.packageBuilder.generateRequirements(n)}}}sanitizeProviderData(e){return{id:e.id,name:e.name,baseUrl:e.baseUrl,authType:e.authType,authHeader:e.authHeader,timeout_ms:e.timeout_ms}}};n(u,"instance");let c=u;export{c as PythonExportService};
