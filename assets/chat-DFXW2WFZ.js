const e="openai-chat",t="OpenAI Chat Completions",n="/v1/chat/completions",o=[{type:"field_transform",config:{field:"response_format",transform:"string_to_object",transformConfig:{objectWrapper:"type"},description:"Wrap response_format string values in {type: value} object"}}],r={promptTokensPath:"usage.prompt_tokens",completionTokensPath:"usage.completion_tokens",totalTokensPath:"usage.total_tokens"},s=[{pattern:"^gpt-5",name:"GPT-5 Series (Reasoning Models)",params:{max_completion_tokens:{type:"integer",description:"Maximum response length including reasoning (1 token ≈ 3-4 characters)",min:1,default:1024,is_output_length:!0,basic:!0},reasoning_effort:{type:"string",description:"Reasoning effort level",enum:["low","medium","high"],basic:!0}},forbidden:["temperature","top_p","frequency_penalty","presence_penalty","stop","logprobs","top_logprobs"]},{pattern:"^o[0-9]",name:"O-Series (Reasoning Models)",params:{max_completion_tokens:{type:"integer",description:"Maximum response length including reasoning (1 token ≈ 3-4 characters)",min:1,default:1024,is_output_length:!0,basic:!0},reasoning_effort:{type:"string",description:"Reasoning effort level",enum:["low","medium","high"],basic:!0}},forbidden:["temperature","top_p","frequency_penalty","presence_penalty","stop","logprobs","top_logprobs"]},{pattern:"^gpt-[234]",name:"GPT 2/3/4 Models",params:{temperature:{type:"number",description:"Creativity level (0 = predictable, 1+ = more creative and varied)",min:0,max:2,default:0,basic:!0},max_completion_tokens:{type:"integer",description:"Maximum response length (1 token ≈ 3-4 characters)",min:1,default:1024,is_output_length:!0,basic:!0},logprobs:{type:"boolean",description:"Include log probabilities",basic:!1,enables_features:{logprobs:{boolean_value:!0}}},top_logprobs:{type:"integer",description:"Number of top log probabilities to return for each token",min:0,max:20,basic:!1}}},{pattern:".*",name:"Other Models",params:{temperature:{type:"number",description:"Creativity level (0 = predictable, 1+ = more creative and varied)",min:0,max:2,default:1,basic:!0},max_completion_tokens:{type:"integer",description:"Maximum response length (1 token ≈ 3-4 characters)",min:1,default:1024,is_output_length:!0,basic:!0},top_p:{type:"number",description:"Top-p (nucleus) sampling",min:0,max:1,default:1,basic:!1},frequency_penalty:{type:"number",description:"Frequency penalty",min:-2,max:2,default:0,basic:!1},presence_penalty:{type:"number",description:"Presence penalty",min:-2,max:2,default:0,basic:!1}}}],a={text:{id:"text",label:"Text",description:"Standard text response",parameters:{},responseTransform:{contentPath:"choices[0].message.content",fallbackPaths:["choices[0].text","message.content"],errorPath:"error.message"}}},i={promptKey:"messages",wrapPrompt:!0,messageRole:"user"},p={id:e,name:t,endpoint:n,requestTransforms:o,usageExtraction:r,modelRules:s,responseModes:a,requestTransform:i};export{p as default,n as endpoint,e as id,s as modelRules,t as name,i as requestTransform,o as requestTransforms,a as responseModes,r as usageExtraction};
